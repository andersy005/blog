{
  
    
        "post0": {
            "title": "MIT 18.S191 Computational Thinking: Introduction to Images",
            "content": ". Note: This notebook is adapted from MIT 18.S191 Introduction to Computational Thinking course (Fall 2020). The course uses Julia Programming language, and I thought it would be fun to follow along while using Python :). . . Note: Check out the lecture video on YouTube . . What is an image, though? . A grid of colored squares called pixels | A color of each pair (i, j) of indices | A discretization | . How can we store an image in the computer? . A 1D array --&gt; vector | A 2D array --&gt; matrix | A 3D array --&gt; tensor | . Load image using scikit-image (skimage) package . import skimage.io import numpy as np import warnings warnings.filterwarnings(&quot;ignore&quot;) . url = &quot;https://i.imgur.com/VGPeJ6s.jpg&quot; philip = skimage.io.imread(url) skimage.io.imshow(philip) . &lt;matplotlib.image.AxesImage at 0x1602b3d90&gt; . type(philip) . numpy.ndarray . philip.dtype . dtype(&#39;uint8&#39;) . philip.shape . (3675, 2988, 3) . Slicing and Indexing . # Show the 100th and 400th pixels skimage.io.imshow(philip[100:101, 400:401, :]) . &lt;matplotlib.image.AxesImage at 0x1622859d0&gt; . skimage.io.imshow(philip[100, 400].reshape(1, 1, 3)) . &lt;matplotlib.image.AxesImage at 0x15d9ba190&gt; . h, w, _ = philip.shape # Extract bottom half of the image # and a tenth of the width until nine tenth of the width into the image head = philip[(h // 2) : h, (w // 10) : ((9 * w) // 10)] skimage.io.imshow(head) . &lt;matplotlib.image.AxesImage at 0x15db11040&gt; . head.shape . (1838, 2391, 3) . philip.shape . (3675, 2988, 3) . Manipulating matrices . An image is just a matrix, so we can manipulate matrices to manipulate the image: . stacked_philip = np.hstack([head, head]) stacked_philip.shape . (1838, 4782, 3) . skimage.io.imshow(stacked_philip) . &lt;matplotlib.image.AxesImage at 0x15e80cb20&gt; . a = np.hstack([head, np.flip(head, axis=1)]) b = np.hstack([np.flip(head, axis=0), np.flip(np.flip(head, axis=1), axis=0)]) c = np.vstack([a, b]) a.shape, b.shape, c.shape . ((1838, 4782, 3), (1838, 4782, 3), (3676, 4782, 3)) . skimage.io.imshow(c) . &lt;matplotlib.image.AxesImage at 0x15fd0c3a0&gt; . Manipulating an image . How can we get inside the image and change it? | There are two possibilities: Modify or mutate numbers inside the array. This is useful when we want to change a small piece | Create a new copy of the array. This is useful when we want to alter everything together | . | . Painting a piece of an image . Let&#39;s paint a corner red | We&#39;ll copy the image first so we don&#39;t destroy the original | . new_phil = head.copy() skimage.io.imshow(new_phil) . &lt;matplotlib.image.AxesImage at 0x15e46c2b0&gt; . Using for loops . %%timeit for i in range(1, 100): for j in range(1, 300): new_phil[i, j] = [255, 0, 0] # Paint this corner red . 42 ms ¬± 301 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each) . skimage.io.imshow(new_phil) . &lt;matplotlib.image.AxesImage at 0x15dfc01c0&gt; . Using element-wise operations via broadcasting . Instead of using for loops, it is better and more efficient to let NumPy do the broadcasting for us during arithmetic operations. . See NumPy broadcasting documentation for detailed explanation . new_phil2 = new_phil.copy() . %%timeit new_phil2[100:200, 1:100] = [0, 255, 0] # Paint this corner green . 52.1 ¬µs ¬± 727 ns per loop (mean ¬± std. dev. of 7 runs, 10000 loops each) . skimage.io.imshow(new_phil2) . &lt;matplotlib.image.AxesImage at 0x15e04c2b0&gt; . Modifying the whole image at once . We can use the same broadcasting trick to modify the whole image at once | Let&#39;s redify the image | . def redify(image): &quot;&quot;&quot;Function that turns a color into just its red component&quot;&quot;&quot; img = image.copy() img[:, :, 1:] = 0 return img skimage.io.imshow(redify(new_phil)) . &lt;matplotlib.image.AxesImage at 0x15d9a08e0&gt; . Transforming an image . Let&#39;s decimate (downsample) philip original image | . def decimate(img, r): &quot;&quot;&quot;Pull every other `r` element in the array&quot;&quot;&quot; return img[0::r, 0::r] . poor_phil = decimate(philip, 5) poor_phil.shape . (735, 598, 3) . skimage.io.imshow(poor_phil) . &lt;matplotlib.image.AxesImage at 0x15e05d340&gt; . Blurring . import skimage.filters import matplotlib.pyplot as plt import panel as pn . # Apply Gaussian blur, creating a new image def blur(image, sigma): blurred_phil = skimage.filters.gaussian( image, sigma=(sigma, sigma), multichannel=True ) return blurred_phil # Create a widget for exploring different sigma values sigma = pn.widgets.Player( name=&quot;Sigma&quot;, start=1, end=30, value=1, loop_policy=&quot;once&quot; ) @pn.depends(sigma) def viewer(sigma): img = blur(image=head, sigma=sigma) ax = skimage.io.imshow(img) plt.title(f&quot;Sigma={sigma}&quot;) fig = ax.get_figure() plt.close(fig) return fig p = pn.Column(&quot;&lt;br&gt; n# Image Viewer n**Select sigma value**&quot;, sigma, viewer) p.save(&quot;apps/blur_explorer.html&quot;, embed=True) . 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 21/29 [01:16&lt;00:21, 2.70s/it] . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 25/29 [01:25&lt;00:09, 2.45s/it] . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 27/29 [01:30&lt;00:04, 2.27s/it] . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . . from IPython.display import HTML, display, display_html display(HTML(&quot;apps/blur_explorer.html&quot;)) . &lt;!DOCTYPE html&gt; Panel . . . . . . . . %load_ext watermark %watermark -d -iv -m -g . skimage 0.17.2 panel 0.9.7 numpy 1.19.1 2020-09-12 compiler : Clang 10.0.1 system : Darwin release : 18.7.0 machine : x86_64 processor : i386 CPU cores : 8 interpreter: 64bit Git hash : ea0b55a307e029a61b7d7ed91a67f3006f513834 .",
            "url": "https://blog.andersonbanihirwe.dev/2020/09/11/MIT-18.S191-Introduction-to-Images.html",
            "relUrl": "/2020/09/11/MIT-18.S191-Introduction-to-Images.html",
            "date": " ‚Ä¢ Sep 11, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Chadwick Boseman",
            "content": "Chadwick Boseman: . Thank you for teaching and showing us how to serve others, while quietly understanding their pain. | . It‚Äôs not every day that St. Jude gets a visit from an Avenger! Thank you @ChadwickBoseman for stopping by to bring joy to our patients and learn more about our lifesaving mission! ‚ù§ üéâ pic.twitter.com/7RwPO7qgPD . &mdash; St. Jude (@StJude) September 12, 2018 Thank you for being the light to so many people, sharing your beautiful spirit and talent with the world! We will miss you so ‚Ä¶. ‚úäüèæ ‚ù§Ô∏è üôÖüèæ‚Äç‚ôÇÔ∏è forever and ever ‚Ä¶ | . Chadwick, throughout it all you found the courage and the will to persevere while uplifting those around you. You didn‚Äôt just play a hero, you were truly one! | . I interviewed Chadwick Boseman in 2017, and to think he was going through cancer while satisfying the physical demands of a Marvel movie...https://t.co/MAB1ZYdcGS pic.twitter.com/eLdear66IU . &mdash; Matt Jacobs (@tarantallegra) August 29, 2020 Repose en paix, Chadwick Boseman. Tu nous manques d√©j√†‚Ä¶ . . ‚ÄúHe who has gone, so we but cherish his memory, abides with us, more potent, nay, more present than the living man.‚Äù ‚Äî Antoine de Saint-Exupery .",
            "url": "https://blog.andersonbanihirwe.dev/2020/08/28/chadwick-boseman.html",
            "relUrl": "/2020/08/28/chadwick-boseman.html",
            "date": " ‚Ä¢ Aug 28, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "From xray to xarray",
            "content": "I started using xarray in the summer of 2017. At the time, the latest version was v0.9.5. Today I learned that in its early stages, xarray was called xray (no ties to x-ray machines :)). Once it gained popularity, the core developers decided it was a good idea to rename the package to a more descriptive/neutral name. The following names were considered: . pandasnd: pandasnd was abandoned because it promised too much, and the core devs didn‚Äôt want to tie xarray to the pandas approach. I wonder if today we would have imported it as: . import pandasnd as pdnd # ü§î # or import pandasnd as pdd # ü§î . | scikit-xray: this one was abandoned because xarray was a better alternative to xray. Here‚Äôs a screenshot of some of the discussions about the name change: . | .",
            "url": "https://blog.andersonbanihirwe.dev/2020/08/15/from-xray-to-xarray.html",
            "relUrl": "/2020/08/15/from-xray-to-xarray.html",
            "date": " ‚Ä¢ Aug 15, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Pandas Dataframe Styling",
            "content": "Preliminaries . import pandas as pd import numpy as np . Create Pandas dataframe with random numbers . df = pd.DataFrame(np.random.randint(1000,100000,size=(15, 4)), columns=list(&#39;ABCD&#39;)) df.head() . A B C D . 0 23709 | 64741 | 23686 | 91188 | . 1 65680 | 82624 | 75561 | 98275 | . 2 92203 | 37811 | 76966 | 38658 | . 3 84619 | 17500 | 5890 | 79898 | . 4 68736 | 50615 | 11204 | 32913 | . For demonstration purposes, let&#39;s create a pivot table . pivot = pd.pivot_table(df, index = [&#39;A&#39;, &#39;B&#39;], values = [&#39;C&#39;, &#39;D&#39;], aggfunc = [&#39;sum&#39;, &#39;mean&#39;]) pivot . sum mean . C D C D . A B . 6405 89881 86138 | 82878 | 86138 | 82878 | . 13323 26813 29352 | 41448 | 29352 | 41448 | . 20598 98959 85063 | 68091 | 85063 | 68091 | . 23709 64741 23686 | 91188 | 23686 | 91188 | . 39780 17193 49436 | 81402 | 49436 | 81402 | . 47835 40995 74271 | 65407 | 74271 | 65407 | . 53481 82855 19225 | 26853 | 19225 | 26853 | . 65680 82624 75561 | 98275 | 75561 | 98275 | . 68736 50615 11204 | 32913 | 11204 | 32913 | . 84619 17500 5890 | 79898 | 5890 | 79898 | . 87051 17441 99532 | 68376 | 99532 | 68376 | . 88006 3533 25610 | 46405 | 25610 | 46405 | . 88274 98894 78792 | 44986 | 78792 | 44986 | . 89596 44772 45295 | 21454 | 45295 | 21454 | . 92203 37811 76966 | 38658 | 76966 | 38658 | . Highlight the maximum and minimum values . pivot.style.highlight_max(color=&#39;green&#39;).highlight_min(color=&#39;red&#39;) . sum mean . C D C D . A B . 6405 89881 86138 | 82878 | 86138 | 82878 | . 13323 26813 29352 | 41448 | 29352 | 41448 | . 20598 98959 85063 | 68091 | 85063 | 68091 | . 23709 64741 23686 | 91188 | 23686 | 91188 | . 39780 17193 49436 | 81402 | 49436 | 81402 | . 47835 40995 74271 | 65407 | 74271 | 65407 | . 53481 82855 19225 | 26853 | 19225 | 26853 | . 65680 82624 75561 | 98275 | 75561 | 98275 | . 68736 50615 11204 | 32913 | 11204 | 32913 | . 84619 17500 5890 | 79898 | 5890 | 79898 | . 87051 17441 99532 | 68376 | 99532 | 68376 | . 88006 3533 25610 | 46405 | 25610 | 46405 | . 88274 98894 78792 | 44986 | 78792 | 44986 | . 89596 44772 45295 | 21454 | 45295 | 21454 | . 92203 37811 76966 | 38658 | 76966 | 38658 | . Add color scales to Pandas DataFrame . pivot.style.background_gradient(cmap=&#39;viridis&#39;) . sum mean . C D C D . A B . 6405 89881 86138 | 82878 | 86138 | 82878 | . 13323 26813 29352 | 41448 | 29352 | 41448 | . 20598 98959 85063 | 68091 | 85063 | 68091 | . 23709 64741 23686 | 91188 | 23686 | 91188 | . 39780 17193 49436 | 81402 | 49436 | 81402 | . 47835 40995 74271 | 65407 | 74271 | 65407 | . 53481 82855 19225 | 26853 | 19225 | 26853 | . 65680 82624 75561 | 98275 | 75561 | 98275 | . 68736 50615 11204 | 32913 | 11204 | 32913 | . 84619 17500 5890 | 79898 | 5890 | 79898 | . 87051 17441 99532 | 68376 | 99532 | 68376 | . 88006 3533 25610 | 46405 | 25610 | 46405 | . 88274 98894 78792 | 44986 | 78792 | 44986 | . 89596 44772 45295 | 21454 | 45295 | 21454 | . 92203 37811 76966 | 38658 | 76966 | 38658 | . Add table captions . pivot.style.set_caption(&#39;Random dataframe with colormaps, with a caption.&#39;) .background_gradient(cmap=&#39;viridis&#39;) . Random dataframe with colormaps, with a caption. sum mean . C D C D . A B . 6405 89881 86138 | 82878 | 86138 | 82878 | . 13323 26813 29352 | 41448 | 29352 | 41448 | . 20598 98959 85063 | 68091 | 85063 | 68091 | . 23709 64741 23686 | 91188 | 23686 | 91188 | . 39780 17193 49436 | 81402 | 49436 | 81402 | . 47835 40995 74271 | 65407 | 74271 | 65407 | . 53481 82855 19225 | 26853 | 19225 | 26853 | . 65680 82624 75561 | 98275 | 75561 | 98275 | . 68736 50615 11204 | 32913 | 11204 | 32913 | . 84619 17500 5890 | 79898 | 5890 | 79898 | . 87051 17441 99532 | 68376 | 99532 | 68376 | . 88006 3533 25610 | 46405 | 25610 | 46405 | . 88274 98894 78792 | 44986 | 78792 | 44986 | . 89596 44772 45295 | 21454 | 45295 | 21454 | . 92203 37811 76966 | 38658 | 76966 | 38658 | . Add color bars to Pandas DataFrame . pivot.style.bar(color=&#39;Green&#39;) . sum mean . C D C D . A B . 6405 89881 86138 | 82878 | 86138 | 82878 | . 13323 26813 29352 | 41448 | 29352 | 41448 | . 20598 98959 85063 | 68091 | 85063 | 68091 | . 23709 64741 23686 | 91188 | 23686 | 91188 | . 39780 17193 49436 | 81402 | 49436 | 81402 | . 47835 40995 74271 | 65407 | 74271 | 65407 | . 53481 82855 19225 | 26853 | 19225 | 26853 | . 65680 82624 75561 | 98275 | 75561 | 98275 | . 68736 50615 11204 | 32913 | 11204 | 32913 | . 84619 17500 5890 | 79898 | 5890 | 79898 | . 87051 17441 99532 | 68376 | 99532 | 68376 | . 88006 3533 25610 | 46405 | 25610 | 46405 | . 88274 98894 78792 | 44986 | 78792 | 44986 | . 89596 44772 45295 | 21454 | 45295 | 21454 | . 92203 37811 76966 | 38658 | 76966 | 38658 | . pd.__version__, np.__version__ . (&#39;1.1.0&#39;, &#39;1.19.1&#39;) . Pandas Styling Documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/style.html .",
            "url": "https://blog.andersonbanihirwe.dev/2020/08/11/adding-style-to-pandas.html",
            "relUrl": "/2020/08/11/adding-style-to-pandas.html",
            "date": " ‚Ä¢ Aug 11, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Dizzy - The Sun and Her Scorch",
            "content": "Two years ago I was introduced to indie music and have never looked back since then. I love indie music for how true to the soul it is. I am thankful for my discovery üòä. . One of my favorite artists is Dizzy, a Canadian indie alt-pop four-piece band. Dizzy recently released their new record The Sun and Her Scorch. It is so beautiful! My top two songs on this record are: . The Magician: which is about Katie Munshaw (who is the vocalist of the band) wanting to magically bring a friend who passed away back to real life. Overall, the song sounds like such an emotional story portrayed beautifully with a little hope. . You be the bird, I‚Äôll be a magician üé∂ . | Roman Candles: which is about the spiraling anxiety and fear one feels whenever they think of having made the wrong career choice. . ‚ÄúThe song is about being nervous and scared that I‚Äôve made the wrong decision in pursuing music as a career. I feel like I‚Äôm being left behind in a way. I think it‚Äôs hard for people to understand music as a career when you‚Äôre not famous. If you‚Äôre not Dua Lipa, people are worried about you. They don‚Äôt realise that it is a career option. It can be hard to relate to people, especially when I‚Äôm not going to work every morning. Christmas dinners are usually pretty awkward. ‚ÄòOh, you‚Äôre still doing music, eh?‚Äô ‚ÄòYes, grandpa!‚Äô‚Äù ‚Äì Katie Munshaw, Apple Music . | . Here‚Äôs the full album on Spotify. Enjoy :) .",
            "url": "https://blog.andersonbanihirwe.dev/2020/08/02/the-sun-and-her-scorch.html",
            "relUrl": "/2020/08/02/the-sun-and-her-scorch.html",
            "date": " ‚Ä¢ Aug 2, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Writing multiple netCDF files in parallel with xarray and dask",
            "content": "Note: Originally published at https://ncar.github.io/xdev/ . A typical computation workflow with xarray consists of: . reading one or more netCDF files into an xarray dataset backed by dask using xr.open_mfdataset() or xr.open_dataset(chunks=...), | applying some transformation to the input dataset, and | saving the resulting output to disk in a netCDF file using xr.to_netcdf(). | The last step (3) can easily lead to a large netCDF file (&gt;=10GB in size). As a result, this step can take a very long time to complete (since it is run in serial), and sometimes may hang. So, to avoid these issues one can use one of the lesser-used but helpful xarray capabilities: the xr.save_mfdataset() function. This function allows users to write multiple datasets to disk as netCDF files simultaneously. The xr.save_mfdataset() function signature looks like this: . xr.save_mfdataset( datasets, paths, mode=&#39;w&#39;, format=None, groups=None, engine=None, compute=True, ) Docstring: Write multiple datasets to disk as netCDF files simultaneously. . Please show me the code . Package imports . import xarray as xr import numpy as np from distributed import Client, performance_report xr.__version__ . &#39;0.15.1&#39; . client = Client() client . Client . Scheduler: tcp://127.0.0.1:65328 | Dashboard: http://127.0.0.1:8787/status | . | Cluster . Workers: 4 | Cores: 8 | Memory: 17.18 GB | . | . Load some toy dataset . ds = xr.tutorial.open_dataset(&#39;rasm&#39;, chunks={&#39;time&#39;: 12}) ds . Show/Hide data repr . . . Show/Hide attributes . . . . xarray.DatasetDimensions:time: 36 | x: 275 | y: 205 | . | Coordinates: (3)time(time)object1980-09-16 12:00:00 ... 1983-08-17 00:00:00long_name :timetype_preferred :intarray([cftime.DatetimeNoLeap(1980-09-16 12:00:00), cftime.DatetimeNoLeap(1980-10-17 00:00:00), cftime.DatetimeNoLeap(1980-11-16 12:00:00), cftime.DatetimeNoLeap(1980-12-17 00:00:00), cftime.DatetimeNoLeap(1981-01-17 00:00:00), cftime.DatetimeNoLeap(1981-02-15 12:00:00), cftime.DatetimeNoLeap(1981-03-17 00:00:00), cftime.DatetimeNoLeap(1981-04-16 12:00:00), cftime.DatetimeNoLeap(1981-05-17 00:00:00), cftime.DatetimeNoLeap(1981-06-16 12:00:00), cftime.DatetimeNoLeap(1981-07-17 00:00:00), cftime.DatetimeNoLeap(1981-08-17 00:00:00), cftime.DatetimeNoLeap(1981-09-16 12:00:00), cftime.DatetimeNoLeap(1981-10-17 00:00:00), cftime.DatetimeNoLeap(1981-11-16 12:00:00), cftime.DatetimeNoLeap(1981-12-17 00:00:00), cftime.DatetimeNoLeap(1982-01-17 00:00:00), cftime.DatetimeNoLeap(1982-02-15 12:00:00), cftime.DatetimeNoLeap(1982-03-17 00:00:00), cftime.DatetimeNoLeap(1982-04-16 12:00:00), cftime.DatetimeNoLeap(1982-05-17 00:00:00), cftime.DatetimeNoLeap(1982-06-16 12:00:00), cftime.DatetimeNoLeap(1982-07-17 00:00:00), cftime.DatetimeNoLeap(1982-08-17 00:00:00), cftime.DatetimeNoLeap(1982-09-16 12:00:00), cftime.DatetimeNoLeap(1982-10-17 00:00:00), cftime.DatetimeNoLeap(1982-11-16 12:00:00), cftime.DatetimeNoLeap(1982-12-17 00:00:00), cftime.DatetimeNoLeap(1983-01-17 00:00:00), cftime.DatetimeNoLeap(1983-02-15 12:00:00), cftime.DatetimeNoLeap(1983-03-17 00:00:00), cftime.DatetimeNoLeap(1983-04-16 12:00:00), cftime.DatetimeNoLeap(1983-05-17 00:00:00), cftime.DatetimeNoLeap(1983-06-16 12:00:00), cftime.DatetimeNoLeap(1983-07-17 00:00:00), cftime.DatetimeNoLeap(1983-08-17 00:00:00)], dtype=object) . | xc(y, x)float64dask.array&lt;chunksize=(205, 275), meta=np.ndarray&gt;long_name :longitude of grid cell centerunits :degrees_eastbounds :xv | Array Chunk . Bytes 451.00 kB | 451.00 kB | . Shape (205, 275) | (205, 275) | . Count 2 Tasks | 1 Chunks | . Type float64 | numpy.ndarray | . | 275 205 | . . | yc(y, x)float64dask.array&lt;chunksize=(205, 275), meta=np.ndarray&gt;long_name :latitude of grid cell centerunits :degrees_northbounds :yv | Array Chunk . Bytes 451.00 kB | 451.00 kB | . Shape (205, 275) | (205, 275) | . Count 2 Tasks | 1 Chunks | . Type float64 | numpy.ndarray | . | 275 205 | . . | . | Data variables: (1)Tair(time, y, x)float64dask.array&lt;chunksize=(12, 205, 275), meta=np.ndarray&gt;units :Clong_name :Surface air temperaturetype_preferred :doubletime_rep :instantaneous | Array Chunk . Bytes 16.24 MB | 5.41 MB | . Shape (36, 205, 275) | (12, 205, 275) | . Count 4 Tasks | 3 Chunks | . Type float64 | numpy.ndarray | . | 275 205 36 | . . | . | Attributes: (11)title :/workspace/jhamman/processed/R1002RBRxaaa01a/lnd/temp/R1002RBRxaaa01a.vic.ha.1979-09-01.ncinstitution :U.W.source :RACM R1002RBRxaaa01aoutput_frequency :dailyoutput_mode :averagedconvention :CF-1.4references :Based on the initial model of Liang et al., 1994, JGR, 99, 14,415- 14,429.comment :Output from the Variable Infiltration Capacity (VIC) model.nco_openmp_thread_number :1NCO :&quot;4.6.0&quot;history :Tue Dec 27 14:15:22 2016: ncatted -a dimensions,,d,, rasm.nc rasm.nc Tue Dec 27 13:38:40 2016: ncks -3 rasm.nc rasm.nc history deleted for brevity | . ds.Tair.data.visualize() . Perform a computation on input dataset . As you can see, our input dataset is chunked along the time dimension. This produces an xarray dataset with 3 chunks. For illustrative purposes, let&#39;s apply some arbitray computation on our dataset: . result = np.sqrt(np.sin(ds) ** 2 + np.cos(ds) ** 2) . result.Tair.data.visualize() . Our resulting xarray dataset has the same dimensions and the same number of chunks as our input dataset. . Create a helper function to split a dataset into sub-datasets . Our main objective is to save this resulting dataset into multiple netCDF files by saving each chunk in its own netCDF file. To accomplish this, we need to create a function that allows us to split a dataset into sub-datasets for each chunk. The following code snippet was adapted from this comment by Stephan Hoyer. . import itertools def split_by_chunks(dataset): chunk_slices = {} for dim, chunks in dataset.chunks.items(): slices = [] start = 0 for chunk in chunks: if start &gt;= dataset.sizes[dim]: break stop = start + chunk slices.append(slice(start, stop)) start = stop chunk_slices[dim] = slices for slices in itertools.product(*chunk_slices.values()): selection = dict(zip(chunk_slices.keys(), slices)) yield dataset[selection] . datasets = list(split_by_chunks(result)) . Let&#39;s confirm that we have three items corresponding to our three chunks: . print(len(datasets)) . 3 . Each item in the returned list consists of an xarray dataset corresponding to each sliced chunk. Now that we have a list of datasets, we need another helper function that will generate a filepath for us when given an xarray dataset as input: . Create a helper function for generating a filepath . def create_filepath(ds, prefix=&#39;filename&#39;, root_path=&quot;.&quot;): &quot;&quot;&quot; Generate a filepath when given an xarray dataset &quot;&quot;&quot; start = ds.time.data[0].strftime(&quot;%Y-%m-%d&quot;) end = ds.time.data[-1].strftime(&quot;%Y-%m-%d&quot;) filepath = f&#39;{root_path}/{prefix}_{start}_{end}.nc&#39; return filepath . Let&#39;s run a small test to make sure this function is working as expected: . create_filepath(datasets[1]) . &#39;./filename_1981-09-16_1982-08-17.nc&#39; . Let&#39;s create a list of paths to which to save each corresponding dataset. . paths = [create_filepath(ds) for ds in datasets] paths . [&#39;./filename_1980-09-16_1981-08-17.nc&#39;, &#39;./filename_1981-09-16_1982-08-17.nc&#39;, &#39;./filename_1982-09-16_1983-08-17.nc&#39;] . Invoke xr.save_mfdataset() . At this point we have two lists: . datasets: List of datasets to save. | paths: List of paths to which to save each corresponding dataset. | We are ready to invoke the xr.save_mfdataset(): . xr.save_mfdataset(datasets=datasets, paths=paths) . Confirm that the output files were properly written . new_ds = xr.open_mfdataset(paths, combine=&#39;by_coords&#39;) new_ds . Show/Hide data repr . . . Show/Hide attributes . . . . xarray.DatasetDimensions:time: 36 | x: 275 | y: 205 | . | Coordinates: (3)xc(y, x)float64dask.array&lt;chunksize=(205, 275), meta=np.ndarray&gt;long_name :longitude of grid cell centerunits :degrees_eastbounds :xv | Array Chunk . Bytes 451.00 kB | 451.00 kB | . Shape (205, 275) | (205, 275) | . Count 10 Tasks | 1 Chunks | . Type float64 | numpy.ndarray | . | 275 205 | . . | yc(y, x)float64dask.array&lt;chunksize=(205, 275), meta=np.ndarray&gt;long_name :latitude of grid cell centerunits :degrees_northbounds :yv | Array Chunk . Bytes 451.00 kB | 451.00 kB | . Shape (205, 275) | (205, 275) | . Count 10 Tasks | 1 Chunks | . Type float64 | numpy.ndarray | . | 275 205 | . . | time(time)object1980-09-16 12:00:00 ... 1983-08-17 00:00:00long_name :timetype_preferred :intarray([cftime.DatetimeNoLeap(1980-09-16 12:00:00), cftime.DatetimeNoLeap(1980-10-17 00:00:00), cftime.DatetimeNoLeap(1980-11-16 12:00:00), cftime.DatetimeNoLeap(1980-12-17 00:00:00), cftime.DatetimeNoLeap(1981-01-17 00:00:00), cftime.DatetimeNoLeap(1981-02-15 12:00:00), cftime.DatetimeNoLeap(1981-03-17 00:00:00), cftime.DatetimeNoLeap(1981-04-16 12:00:00), cftime.DatetimeNoLeap(1981-05-17 00:00:00), cftime.DatetimeNoLeap(1981-06-16 12:00:00), cftime.DatetimeNoLeap(1981-07-17 00:00:00), cftime.DatetimeNoLeap(1981-08-17 00:00:00), cftime.DatetimeNoLeap(1981-09-16 12:00:00), cftime.DatetimeNoLeap(1981-10-17 00:00:00), cftime.DatetimeNoLeap(1981-11-16 12:00:00), cftime.DatetimeNoLeap(1981-12-17 00:00:00), cftime.DatetimeNoLeap(1982-01-17 00:00:00), cftime.DatetimeNoLeap(1982-02-15 12:00:00), cftime.DatetimeNoLeap(1982-03-17 00:00:00), cftime.DatetimeNoLeap(1982-04-16 12:00:00), cftime.DatetimeNoLeap(1982-05-17 00:00:00), cftime.DatetimeNoLeap(1982-06-16 12:00:00), cftime.DatetimeNoLeap(1982-07-17 00:00:00), cftime.DatetimeNoLeap(1982-08-17 00:00:00), cftime.DatetimeNoLeap(1982-09-16 12:00:00), cftime.DatetimeNoLeap(1982-10-17 00:00:00), cftime.DatetimeNoLeap(1982-11-16 12:00:00), cftime.DatetimeNoLeap(1982-12-17 00:00:00), cftime.DatetimeNoLeap(1983-01-17 00:00:00), cftime.DatetimeNoLeap(1983-02-15 12:00:00), cftime.DatetimeNoLeap(1983-03-17 00:00:00), cftime.DatetimeNoLeap(1983-04-16 12:00:00), cftime.DatetimeNoLeap(1983-05-17 00:00:00), cftime.DatetimeNoLeap(1983-06-16 12:00:00), cftime.DatetimeNoLeap(1983-07-17 00:00:00), cftime.DatetimeNoLeap(1983-08-17 00:00:00)], dtype=object) . | . | Data variables: (1)Tair(time, y, x)float64dask.array&lt;chunksize=(12, 205, 275), meta=np.ndarray&gt; | Array Chunk . Bytes 16.24 MB | 5.41 MB | . Shape (36, 205, 275) | (12, 205, 275) | . Count 9 Tasks | 3 Chunks | . Type float64 | numpy.ndarray | . | 275 205 36 | . . | . | Attributes: (0) | . try: xr.testing.assert_identical(result, new_ds) except AssertionError: print(&#39;The datasets are not identical!&#39;) else: print(&#39;The datasets are identical!&#39;) . The datasets are identical! . There you have it üéâüéâüéâüéâ . Conclusion . This article was inspired by a problem that my colleague was running into. I thought it would be useful to document this fix in a blog post. Many thanks to Deepak Cherian for pointing me to some relevant xarray issues and to the @NCAR/xdev team for their valuable and critical feedback on drafts of this article along the way. .",
            "url": "https://blog.andersonbanihirwe.dev/2020/06/29/writing-multiple-netcdf-files-in-parallel-wit-xarray-and-dask.html",
            "relUrl": "/2020/06/29/writing-multiple-netcdf-files-in-parallel-wit-xarray-and-dask.html",
            "date": " ‚Ä¢ Jun 29, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "A Knitting Weekend",
            "content": "For the last few months, I&#39;ve become a huge fan of the quiet weekend, the weekend when no plan is the plan, and you are in no hurry at all. I woke up on Saturday morning, and I decided that this weekend was going to be a quiet one. After watching a late morning football game from the English Premier League -- yup, I refuse to call it soccer üòÄ -- I decided to write some code-nothing big, just a few lines of codes to improve my experience with Dash, a tool that I&#39;ve been using for a few months. Dash is a MacOS application that offers offline documentation for hundreds of programming languages, fraweworks and libraries. Dash is awesome for offline programming when using spotty WiFi or on long-haul flights. While it is free as in beer, it&#39;s one of the programs that are worth paying for. . The Problem . I&#39;ve been using Dash for quite some time and have been really satisfied with the preset selection of documentation sets. At some point, I started running into scenarios when I was using frameworks/libraries whose documentations aren&#39;t available for Dash. So, my objective was to generate documentation sets for these libraries myself. . The Solution: Doc2dash and Sphinx To The Rescue . doc2dash is a great tool for converting HTML documentation to Dash&#39;s format. doc2dash takes the output of Sphinx documentation and converts it to a Dash documentation package. . Show Me The Code! . In Just a few steps, I was able to build a docset for Zarr using dash2doc: . Create new conda environment and install dependencies | . $ conda create -n dash-docs -c conda-forge doc2dash sphinx-rtd-theme numpydoc sphinx-issues $ conda activate dash-docs . Pull down the project form GitHub and build the documentation | . $ git clone https://github.com/zarr-developers/zarr-python.git $ cd zarr-python $ python -m pip install -e . $ cd docs $ make html . Finally, run doc2dash with a few commands: | . $ doc2dash --name zarr --index-page index.html --enable-js --add-to-dash _build/html . And with that I had a shiny new Zarr docset in Dash: . . Once I was confident that I understood how to do this for one project, I decided to automate this process for a bunch of other projects. By dinner time (on Saturday) I felt like I was on the right track. After a few more hours on Sunday and Monday, I had a GitHub repository with continuous integration via CircleCI up and running, and had a dozen docsets generated: . . . Was It Worth Fifteen or Twenty Hours of My Life? . I think so: I took pleasure in doing something that I knew how to do and in creating something that would fit my needs perfectly. . Sitting in a chair in front a computer screen while listening to MIX 100 Denver Radio station üòÄ, I was reminded of the intriguing parallels between knitting and programming: . &quot;Knitting is Coding, and Yarn is a Programmable material.&quot; . On quiet weekends like this I write code for fun, and Visual Studio Code &amp; Python are my yarn and needles. . . Note: Sometimes a quiet weekend is an absolute gift! .",
            "url": "https://blog.andersonbanihirwe.dev/2020/01/21/a-knitting-weekend.html",
            "relUrl": "/2020/01/21/a-knitting-weekend.html",
            "date": " ‚Ä¢ Jan 21, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Reflecting on SciPy 2019",
            "content": "It‚Äôs been a week since SciPy 2019 conference came to a close. I had the pleasure of speaking at SciPy 2019 about interactive supercomputing with Dask and Jupyter. I am so thankful for the chance to speak in the Earth, Ocean, Geo and Atmospheric Science track! . Here‚Äôs a recording of my talk: . As I look back at my first SciPy, I can say that it was the most productive conference that I‚Äôve been to so far. So, I wanted to take this opportunity to summarize my key takeaways. . 1. The Awesomeness of SciPy Community . The biggest highlight of SciPy for me was the community. What an authentic, super-talented, enthusiastic and welcoming community! I met and interacted with a lot of amazing people that I only knew via Github prior the conference :). The community is really what makes SciPy such a special conference. . 2. Invisible Work in Open Source Software Projects . Because so much of the work in OSS is tracked on public platforms, it is easy to forget about the work that takes place outside of them. . This quote by Stuart really hit home. For the last few months, I‚Äôve become a core maintainer of a few projects, and one of the things I‚Äôve learned so far as a user, a contributor to, and core maintainer of an open source project is that it‚Äôs really important to recognize that the github repository does not fully represent a project. There‚Äôs quite a lot of crucial, untracked work that goes on in the background such as: . . You can watch the full key note here. . 3. Tell People When You Appreciate Their Work . Another key lessons I learned is from Carol Willing‚Äôs keynote is that research shows that meeting someone who has befitted from your work can double your effort and triple your productivity: . . 4. We Should Think About Open Source Standards . Matthew Rocklin gave a great talk on refactoring the SciPy ecosystem for heterogeneous computing. . My takeaway from his talk is that . The Python scientific community fractures when we develop competing packages that are not compatible with each other and thrives when we develop tools based on common standards. . As software developers, we should try to follow already existing standards when we can, because standards level the playing field in so many ways: . new technologies can quickly compete | new developers can quickly engage | incentivize support (e.g. GitHub renders .ipynb since Jupyter notebooks are built on a widely used standard) | . 5. Jupyter Notebooks Come Alive with Jupyter Widgets . One of my favorite SciPy 2019 talks is Martin and Maarten‚Äôs talk on Dashboarding with Jupyter Notebooks, Voil√† and Widgets. It is really fun and full of cool stuffs. . I am looking forward to making my own widgets and dashboards! And exclaiming voil√†! after I deploy them! . 6. Maslow‚Äôs Hierarchy of Software Needs . Stan‚Äôs talk on How to Accelerate an Existing Codebase with Numba is also among my favorites. The Key lesson from this talk is the following: . Be honest with yourself about where your project is in this hierarchy. If your program doesn‚Äôt run, there‚Äôs no point in making it faster. . . 7. Final Thoughts . I left SciPy 2019 feeling: . very thankful that so many amazing people are able to come out and collaborate on the projects that are so vital to the Python scientific ecocystem! | grateful to be part of this community. | inspired to keep contributing to this community. | so excited about the technology, the science, and the people. | . Thank you so much to the SciPy Conference organizers for putting on a great conference. .",
            "url": "https://blog.andersonbanihirwe.dev/2019/07/21/scipy-2019.html",
            "relUrl": "/2019/07/21/scipy-2019.html",
            "date": " ‚Ä¢ Jul 21, 2019"
        }
        
    
  
    
        ,"post8": {
            "title": "Summary of TVM -- End-to-End Optimization Stack for Deep Learning",
            "content": "Abstract . Paper link: https://arxiv.org/abs/1802.04799 . Scalable frameworks, such as TensorFlow, MXNet, Caffe, and PyTorch are optimized for a narrow range of serve-class GPUs. | Deploying workloads to other platforms such as mobile phones, IoT, and specialized accelarators(FPGAs, ASICs) requires laborious manual effort. | TVM is an end-to-end optimization stack that exposes: . graph-level | operator-level optimizations | . &gt; to provide performance portability to deep learning workloads across diverse hardware back-ends. . | . Introduction . The number and diversity of specialized deep learning (DL) accelerators pose an adoption challenge . They introduce new hardware abstractions that modern compilers and frameworks are ill-equipped to deal with. | . | Providing support in various DL frameworks for diverse hardware back-ends in the present ad-hoc fashion is unsustainable. . | Hardware targets significantly diverge in terms of memory organization, compute, etc.. . | . . The Goal: easily deploy DL workloads to all kinds of hardware targets, including embedded devives, GPUs, FPGAs, ASCIs (e.g, the TPU). . | Current DL frameworks rely on a computational graph intermediate representation to implement optimizations such as: . auto differentiation | dynamic memory management | . | Graph-level optimizations are often too high-level to handle hardware back-end-specific operator transformations. . | Current operator-level libraries that DL frameworks rely on are: . too rigid | specialized | . &gt; to be easily ported across hardware devices . | To address these weaknesses, we need a compiler framework that can expose optimization opportunities across both . graph-level and | operator-level | . &gt; to deliver competitive performance across hardware back-ends. . | . Four fundamental challenges at the computation graph level and tensor operator level . High-level dataflow rewriting: . Different hardware devices may have vastly different memory hierarchies. . | Enabling strategies to fuse operators and optimize data layouts are crucial for optimizing memory access. . | . | Memory reuse across threads: . Modern GPUs and specialized accelerators ahve memory that can be shared across compute cores. | Traditional shared-nothing nested parallel model is no longer optimal. | Cooperation among threads on shared memory loaded is required for optimized kernels. | . | Tensorized compute intrinsics: . The latest hardware provides new instructions that go beyond vector operations like the GEMM operator in TPU or the tensor core in NVIDIA&#39;s Volta. | Consequently, the scheduling procedure must break computation into tensor arithmetic intrinsics instead of scalar or vector code. | . | Latency Hiding . Traditional architectures with simultaneous multithreading and automatically managed caches implicitly hide latency in modern CPUs/GPUs. | Specialized accelerator designs favor learner control and offload most of the scheduling complexity to the compiler stack. | Still, scheduling must be peformed carefully to hide memory access latency. | . | TVM: An End-to-End Optimization Stack . An end-to-end optimizing compiler stack to lower and fine-tune DL workloads to diverse hardware back-ends. | Designed to separate: the algorithm description | schedule | hardware interface | . | This separation enables support for novel specialized accelerators and their corresponding new intrinsics. | TVM presents two optimization layers: a computation graph optimization layer to address: High-level dataflow rewriting | . | a tensor optimization layer with new schedule primitives to address: memory reuse across threads | tensorized compute intrinsics | latency hiding | . | . | . Optimizing Computational Graphs . Computational Graph . Computational graphs are a common way to represent programs in DL frameworks. | They provide a global view on computation tasks, yet avoid specifying how each computation task needs to be implemented. | . Operator Fusion . An optimization that can greatly reduce execution time, particulary in GPUs and specialized accelerators. | The idea is to combine multiple operators together into a single kernel without saving the intermediate results back into global memory | . . Four categories of graph operators: . Injective (one-to-one map) | Reduction | Complex-out-fusable (can fuse element-wise map to output) | Opaque (cannot be fused) | . . Data Layout Transformation . Tensor operations are the basic operators of computational graphs | They can have divergent layout requirements across different operations | Optimizing data layout starts with specifying the preferred data layout of each operator given the constraints dictating their implementation in hardware. | . . Limitations of Graph-Level Optimizations . They are only as effective as what the operator library provides. | Currently, the few DL frameworks that support operator fusion require the operator library to provide an implementation of the fused patterns. With more network operators introduced on a regular basis, this approach is no longer sustainable when targeting an increasing number of hardware back-ends. | . | It is not feasible to handcraft operator kernels for this massive space of back-end specific operators TVM provides a code-generation approach that can generate tensor operators. | . | . Optimizing Tensor Operations . Tensor Expression Language . TVM introduces a dataflow tensor expression language to support automatic code generation. | Unlike high-level computation graph languages, where the implementation of tensor operations is opaque, each operation is described in an index formula expression language. | . . TVM tensor expression language supports common arithmetic and math operations found in common language like C. | TVM explicitly introduces a commutative reduction operator to easily schedule commutative reductions across multiple threads. | TVM further introduces a high-order scan operator that can combine basic compute operators to form recurrent computations over time. | . Schedule Space . Given a tensor expression, it is challenging to create high-performance implementations for each hardware back-end. | Each optimized low-level program is the result of different combinations of scheduling strategies, imposing a large burden on the kernel writer. | TVM adopts the principle of decoupling compute descriptions from schedule optimizations. | Schedules are the specific rules that lower compute descriptions down to back-end-optimized implementations. | . . . Nested Parallelism with Cooperation . Parallel programming is key to improving the efficiency of compute intensive kernels in deep learning workloads. | Modern GPUs offer massive parallelism . &gt; Requiring TVM to bake parallel programming models into schedule transformations . | Most existing solutions adopt a parallel programming model referred to as nested parallel programs, which is a form of fork-join parallelism. . | TVM uses a parallel schedule primitive to parallelize a data parallel task Each parallel task can be further recursively subdivided into subtasks to exploit the multi-level thread hierarchy on the target architecture (e.g, thread groups in GPU) | . | This model is called shared-nothing nested parallelism . One working thread cannot look at the data of its sibling within the same parallel computation stage. | Interactions between sibling threads happen at the join stage, when the subtasks are done and the next stage can consume the data produced by the previous stage. | This programming model does not enable threads to cooperate with each other in order to perform collective task within the same parallel stage. | . | A better alternative to the shared-nothing approach is to fetch data cooperatively across threads . This pattern is well known in GPU programming using languages like CUDA, OpenCL and Metal. | It has not been implemented into a schedule primitive. | . | TVM introduces the concept of memory scopes to the schedule space, so that a stage can be marked as shared. Without memory scopes, automatic scope inference will mark the relevant stage as thread-local. | Memory scopes are useful to GPUs. | Memory scopes allow us to tag special memory buffers and create special lowering rules when targeting specialized deep learning accelerators. | . | . . Tensorization: Generalizing the Hardware Interface . Tensorization problem is analogous to the vectorization problem for SIMD architectures. | Tensorization differs significantly from vectorization The inputs to the tensor compute primitives are multi-dimensional, with fixed or variable lengths, and dictate different data layouts. | Cannot resort to a fixed set of primitives, as new DL accelerators are emerging with their own flavors of tensor instructions. | . | To solve this challenge, TVM separates the hardware interface from the schedule: TVM introduces a tensor intrinsic declaration mechanism | TVM uses the tensor expression language to declare the behavior of each new hardware intrinsic, as well as the lowering rule associated to it. | TVM introduces a tensorize schedule primitive to replace a unit of computation with the corresponding tensor intrinsics. | The compiler matches the computation pattern with a hardware declaration, and lowers it to the corresping hardware intrinsic. | . | . Compiler Support for Latency Hiding . Latency Hiding: refers to the process of overlapping memory operations with computation to maximize memory and compute utilization. | It requires different different strategies depending on the hardware back-end that is being targeted. | On CPUs, memory latency hiding is achieved implicitly with simultaneous multithreading or hardware prefetching techniques. | GPUs rely on rapid context switching of many wraps of threads to maximize the utilization of functional units. | TVM provides a virtual threading schedule primitive that lets the programmer specify a high-level data parallel program that TVM automatically lowers to a low-level explicit data dependence program. | . Code Generation and Runtime Support . Code Generation . For a specific tuple of data-flow declaration, axis relation hyper-graph, and schedule tree, TVM can generate lowered code by: iteratively traversing the schedule tree | inferring the dependent bounds of the input tensors (using the axis relation hyergraph) | generating the loop nest in the low-level code | . | The code is lowered to an in-memory representation of an imperative C style loop program. | TVM reuses a variant of Halide&#39;s the loop program data structure in this process. | TVM reuses passes from Halide for common lowering primitives like storage flattening and unrolling, and add GPU/accelerator-specific transformations such as: synchronization point detection | virtual thread injection | module generation | . | . | Finally, the loop program is transformed into LLVM or CUDA/Metal/OpenCL source code. | . Runtime Support . For GPU programs, TVM builds the host and device modules separately and provide a runtime module system that launch kernels using corresponding driver APIs. | . Remote Deployment Profiling . TVM includes infrastructure to make profiling and autotuning easier on embedded devices. | Traditionally, targeting an embedded device for tuning requires: . cross-compiling on the host side, | copying to the target device, | and timing the execution | . | TVM provides remote function call support. Through the RPC interface: . TVM compiles the program on a host compiler | it uploads to remote embedded devices | it runs the funcion remotely, | and it accesses the results in the same script on the host. | . | . . Conclusion . TVM provides an end-to-end stack to solve fundamental optimization challenges across a diverse set of hardware back-ends. | TVM can encourage more studies of programming languages, compilation, and open new opportunities for hardware co-design techniques for deep learning systems. | .",
            "url": "https://blog.andersonbanihirwe.dev/2018/05/10/TVM-paper-summary.html",
            "relUrl": "/2018/05/10/TVM-paper-summary.html",
            "date": " ‚Ä¢ May 10, 2018"
        }
        
    
  
    
        ,"post9": {
            "title": "Play interactively with C++: STL Sequence Containers",
            "content": "The Standard Template Library (STL) is a programmer&#39;s dream. It offers efficient ways to: . store | access | manipulate | view data | . and is designed for maximum extensibility. Once a programmer has gotten over the initial syntax hurdles, they quickly learn to appreciate the STL&#39;s sheer power and flexibility. . Overview of STL . Containers: a collection of container classes. For example: . map: an associative collection of key/value pairs | vector: a growing list of elements. | . | Iterators: objects that view and modify ranges of stored data. Each STL container exports iterators. Iterators have a common interface, allowing the programmer to write code that operates on data stored in arbitrary containers. . | Algorithms: functions that operate over ranges of data specified by iterators. . | Adapters: objects which transform an object from one form into another. For instance: . stack adapter transforms a regular vector or list into a LIFO (Last In First Out) container. | istream_iterator transforms a standard C++ stream inot an STL iterator. | . | Functions: facilities for creating and modifying functions at runtime. . | Allocators: allow clients of the container classes to customize how memory is allocated and deallocated, either for diagnostic or performance reasons. . | . Vectors . A std::vector is an object that represents a sequence of elements. The elements in a vector are indexed, meaning that they can have a well-defined position in the sequence. . #include &lt;iostream&gt; #include &lt;vector&gt; // Necessary to use vector #include &lt;string&gt; #include &lt;sstream&gt; . // Create a vector containing integers std::vector&lt;int&gt; v = {7, 10, 15, 8, 98, 0}; . Add two more integers to vector using push_back() . v.push_back(45); v.push_back(56); . Helper function to iterate and print values of vector . void PrintVector(std::vector&lt;int&gt;&amp; v){ for(int n : v){ std::cout &lt;&lt; n &lt;&lt; &#39; &#39;; } std::cout &lt;&lt; std::endl; } . PrintVector(v) . 7 10 15 8 98 0 45 56 . The storage of the vector is handled automatically, being expanded and contracted as needed. Vectors usually occupy more space than static arrays, because more memory is allocated to handle future growth. This way a vector does not need to reallocate each time an element is inserted, but only when the additional memory is exhausted. . Grow the vector, setting new elements to 0 . v.resize(10); PrintVector(v) . 7 10 15 8 98 0 45 56 0 0 . The total amount of allocated memory can be queried using capacity() function. Extra memory can be returned to the system via a call to shrink_to_fit() . capacity(): returns the number of element that can be held in currently allocated storage . v.capacity() . 12 . shrink_to_fit(): reduces memory usage by freeing unused memory . v.shrink_to_fit() . v.capacity() . 10 . resize(): changes the number of elements stored. E.g; Grow the vector, setting new elements to 100** . v.resize(13, 100); PrintVector(v); . 7 10 15 8 98 0 45 56 0 0 100 100 100 . v.capacity() . 20 . pop_back(): removes the last element . v.pop_back(); PrintVector(v); . 7 10 15 8 98 0 45 56 0 0 100 100 . NOTE: for detailed information about std::vector, check out the C++ reference. . deque . There are several aspects of the vector that can be troublesome in certain applications. In particular, the vector is only designed to grow in one direction; calling push_back() inserts elements at the end of the vector, and resize() always appends elements to the end. . A std:deque is an indexed sequence container that allows fast insertion and deletion at both its beginning and its end. As opposed to std:vector, the elements of a deque are not stored contiguously: typical implementations use a sequence of individually allocated fixed-size arrays, with additional bookkeeping, which means indexed access to deque must perform two pointer dereferences, compared to vector&#39;s indexed access which performs only one. . #include &lt;iostream&gt; #include&lt;deque&gt; . Create a deque (pronounced &quot;deck&quot;) containing integers . std::deque&lt;int&gt; d = {45, 48, 40, 79}; . void PrintDeque(std::deque&lt;int&gt;&amp; d){ for(int n : d){ std::cout &lt;&lt; n &lt;&lt; &#39; &#39;; } std::cout &lt;&lt; std::endl; } . PrintDeque(d); . 45 48 40 79 . Add an integer to the beginning and end of the queue . d.push_front(13); d.push_back(25); . PrintDeque(d); . 13 45 48 40 79 25 . A deque can do everything a vector can do and also unlike a vector, it is possible (and fast) to push_front and pop_front . NOTE: for detailed information about std::deque, check out the C++ reference or just type ?std::queue in a notebook cell to get live documentation. . Container Adapters . How can we implement stack and queue using the containers we have? . Stack . Stack is a type of container adapters with LIFO (Last In First Out) or FILO (First in, Last Out) data structure, where a new element is added at one end and (top) an element is removed from that end only. . Stack: just limit the functionality of a vector/deque to only allow push_back and pop_back. . The functions associated with stack are: . empty() -- Returns whether the stack is empty | size() -- Returns the size of the stack | top() -- Returns a reference to the top most element of the stack | push(g) -- Adds the element &#39;g&#39; at the top of the stack | pop() -- Deletes the top most element of the stack | . #include &lt;stack&gt; . Create a stack of integers . std::stack&lt;int&gt; s; . Helper function to show the stack . void ShowStack(std::stack&lt;int&gt; mystack){ std::stack&lt;int&gt; s = mystack; while (!s.empty()){ std::cout &lt;&lt; &#39; t&#39; &lt;&lt; s.top(); s.pop(); } std::cout &lt;&lt; std::endl; } . ShowStack(s) . . Push some elements onto the stack . s.push(10); s.push(30); s.push(20); s.push(5); . ShowStack(s) . 5 20 30 10 . Get stack size . s.size() . 4 . Get the top of the stack . s.top() . 5 . Pop top of the stack . s.pop(); . ShowStack(s) . 20 30 10 . Queue . Queue is a type of container adaptors which operate in a first in first out (FIFO) type of arrangement. Elements are inserted at the back (end) and are deleted from the front. . The functions supported by queue are: . empty() -- Returns whether the queue is empty | size() -- Returns the size of the queue | front() -- Returns a reference to the first element of the queue. | back() -- Returns a reference to the last element of the queue | push(g) -- Adds the element g at the end of the queue | pop() -- Deletes the first element of the queue. | . #include &lt;queue&gt; . Create an empty queue . std::queue&lt;int&gt; q; . Helper function to show content of the queue . void ShowQueue(std::queue&lt;int&gt; myqueue){ std::queue&lt;int&gt; q = myqueue; while (!q.empty()){ std::cout &lt;&lt; &#39; t&#39; &lt;&lt; q.front(); q.pop(); } std::cout &lt;&lt; std::endl; } . ShowQueue(q) . . Add some elements to the queue . q.push(10); q.push(20); . ShowQueue(q) . 10 20 . Get queue size . q.size() . 2 . Get front of the queue . q.front() . 10 . Get back of the queue . q.back() . 20 . Pop front of the queue . q.pop() . ShowQueue(q) . 20 .",
            "url": "https://blog.andersonbanihirwe.dev/2018/01/23/play-interactively-with-cpp-sequence-containers.html",
            "relUrl": "/2018/01/23/play-interactively-with-cpp-sequence-containers.html",
            "date": " ‚Ä¢ Jan 23, 2018"
        }
        
    
  
    
        ,"post10": {
            "title": "Play interactively with C++: Streams",
            "content": "A stream is an abstraction for input/output. You can think of it as a source (input) or destination (output) of characters of indefinite length. . How do you write data to a file in C++? . C++ provides a header filed called &lt;fstream&gt; (file stream) that exports the ifstream and ofstream types, streams that perform file I/O. Input/output stream class to operate on files. . Objects of this class maintain a filebuf object as their internal stream buffer, which performs input/output operations on the file they are associated with (if any). . #include &lt;fstream&gt; // std::fstream . std::fstream fs; fs.open (&quot;test.txt&quot;, std::fstream::in | std::fstream::out | std::fstream::app); fs &lt;&lt; &quot;More lorem ipsum....&quot;; // write to the created file fs.close(); // Close the stream . !cd iofiles &amp;&amp; ls &amp;&amp; cat test.txt . fs.open (&quot;test.txt&quot;, std::fstream::in | std::fstream::out | std::fstream::app); // Re-open the stream fs &lt;&lt; &quot;Last but not the least lorem ipsum!!&quot;; // Write some more to the file fs.close(); // Close the stream . !cat test.txt . More lorem ipsum....Last but not the least lorem ipsum!! . Stream Manipulators . What does the setw manipulator do? . setw(n) sets the minimum width of the input for the next stream operation. If the data doesn&#39;t meet the minimum field requirement, it is padded with the default fill character until it is proper size. . #include &lt;iostream&gt; . std::cout &lt;&lt; &quot;Output: &quot; &lt;&lt; 10 &lt;&lt; std::endl; . Output: 10 . std::cout &lt;&lt; &quot;Output: &quot; &lt;&lt; std::setw(5) &lt;&lt; 10 &lt;&lt; std::endl; . Output: 10 . What does the boolalpha manipulator do? . boolalpha determines whether or not the stream should output boolean values as 1 and 0 or as &quot;true&quot; and &quot;false&quot;. The opposite manipulator is noboolalpha, which reverses this behaviour. . std::cout &lt;&lt; true &lt;&lt; std::endl; . 1 . std::cout &lt;&lt; std::boolalpha&lt;&lt; true &lt;&lt; std::endl; . true . What do hex, dec, oct manipulator do? . They set the radix on the stream to either octal (base 8), decimal (base 10), or hexadecimal (base 16). This can be used either to format output or change the base for input. . std::cout &lt;&lt; 10 &lt;&lt; std::endl; . 10 . std::cout &lt;&lt; std::dec &lt;&lt; 10 &lt;&lt; std::endl; . 10 . std::cout &lt;&lt; std::oct &lt;&lt; 10 &lt;&lt; std::endl; . 12 . std::cout &lt;&lt; std::hex &lt;&lt; 10 &lt;&lt; std::endl; . a . What is stream failure? How do you check for it? . Because stream operations often involve transforming data from one form into another, stream operations are not always guaranteed to succeed. . Let&#39;s use the previously created test.txt file to demonstrate stream failure. . For the following program: . If the file test.txt contains NUM_INTS consecutive integer values, then this code will work correctly. | If we try to read stream data of one type into a variable of another type, rather than crashing the program or filling the variable with garbage data, the stream fails by entering an error state and the value of the variable will not change. Once the stream is in this error state, any subsequent read or write operations will automatically and silently fail which can be a serious problem . | . To check if a stream is in an erroneous state, we use the .fail() member function. . fs.open (&quot;test.txt&quot;, std::fstream::in | std::fstream::out | std::fstream::app); . int NUM_INTS = 5; . for(int i = 0; i &lt; NUM_INTS; i++){ int value; fs &gt;&gt; value; /*..... Do something Here ....*/ if(fs.fail()) break; std::cout &lt;&lt; value &lt;&lt; std::endl; } . As you can see, when the program realizes that the file contains text and not integers, it breaks out of the loop without doing any subsequent operation. . What is a stringstream? . In C++, you can&#39;t add numbers to strings and when you can, it&#39;s almost certainly won&#39;t do what you expected. . One solution to this problem is to use another kind of stream object known as a stringstream, exported by sstreamheader. Like console streams and file streams, stringstreams are stream objects. Instead of reading or writing data to an external source, stringstreams store data in temporary string buffers. . #include &lt;sstream&gt; // std::stringstream, std::stringbuf . std::stringstream ss; ss &lt;&lt; &quot;Hello World! It&#39;s &quot; &lt;&lt; 135 &lt;&lt; std::endl; . Once you have put data into a stringstream, you can retrieve the string you have created using the .str() member function. . std::string s = ss.str(); std:: cout &lt;&lt; s &lt;&lt; std::endl; . Hello World! It&#39;s 135 . stringstreams are an example of an iostream, a stream that can perform both input and output, You can both insert data inot a stringstream to convert the data to string and extract data from a stringstream to convert string data into a different format. For example: . std::stringstream myConverter; int myInt; std::string myString; double myDouble; . std::cout &lt;&lt; myInt &lt;&lt; std::endl; std::cout &lt;&lt; myString &lt;&lt; std::endl; std::cout &lt;&lt; myDouble &lt;&lt; std::endl; . 0 0 . myConverter &lt;&lt; &quot;5 World 3.14&quot;; // Insert string data . @0x10d521080 . myConverter &gt;&gt; myInt &gt;&gt; myString &gt;&gt; myDouble; // Extract mixed data . @0x10d521070 . std::cout &lt;&lt; myInt &lt;&lt; std::endl; std::cout &lt;&lt; myString &lt;&lt; std::endl; std::cout &lt;&lt; myDouble &lt;&lt; std::endl; . 5 World 3.14 . Write a function that converts an int into a string . std::string intToStr(int myInt){ std::stringstream ss; ss &lt;&lt; myInt; std::string s = ss.str(); return s; } . std::string str = intToStr(5); std::cout &lt;&lt; str &lt;&lt; std::endl; . 5 . Draw a tringle by drawing a single character on one line, then three on the next, ... . void DrawTriangle(int numRows){ for(int i = 1 ; i &lt;= numRows; i++) { //Print a field of spaces equal to the number of rows minus half the width of the triangle, plus 2 spaces //between the row number and the triangle, std::cout &lt;&lt; std::setfill(&#39; &#39;) &lt;&lt; std::setw(numRows - i + 2) &lt;&lt; &quot; &quot;; //Print a field of pipes equal to the number of row number*2 then subtract 1 to keep the width odd and centered. std::cout &lt;&lt; std::setfill(&#39;#&#39;) &lt;&lt; std::setw((i * 2 - 1)) &lt;&lt; &quot;#&quot;; std::cout &lt;&lt; std::endl; } } . DrawTriangle(5) . # ### ##### ####### ######### . DrawTriangle(8) . # ### ##### ####### ######### ########### ############# ############### . DrawTriangle(15); . # ### ##### ####### ######### ########### ############# ############### ################# ################### ##################### ####################### ######################### ########################### ############################# .",
            "url": "https://blog.andersonbanihirwe.dev/2018/01/20/play-interactively-with-cpp-streams.html",
            "relUrl": "/2018/01/20/play-interactively-with-cpp-streams.html",
            "date": " ‚Ä¢ Jan 20, 2018"
        }
        
    
  
    
        ,"post11": {
            "title": "Play interactively with C++: Xeus-Cling",
            "content": "This is the 1st installment of a new series called Play interactively with C++. Every week or so, I‚Äôll be summarizing and exploring Standard C++ Programming in Jupyter notebook using xeus-cling. . The source code (in notebook format) for this series can be found here. . xeus-cling is a Jupyter kernel for C++ based on the C++ interpreter cling and the native implementation of the Jupyter protocol xeus. . In this installment, I will go over how to get started with playing with C++ Programming interactively. . I am going to show you how to setup your environment. This blog post makes the following assumptions: . The reader has some experience with Jupyter notebooks. | Familiarity with conda. | Note: Credit goes to Uwe for his excellent blog post on how to setup C++ environment for Apache Arrow . Getting Started . To quote xeus-cling project: . It is preferable to install xeus-cling in a fresh conda environment. It is also needed to use a miniconda installation because with anaconda you can have a conflict with the zeromq library which is already installed with anaconda. . As a start, we create a conda environment with all non-C++ dependencies and also install Jupyter Lab from conda-forge. . # Create a new conda environment conda create -n xeus python=3.6 jupyterlab -c conda-forge source activate xeus . Finally, let&#39;s install the actual interactive environment. For the C++ support, we install the interactive C++ compiler cling and the C++ kernel for Jupyter Notebook xeus-cling from the QuantStack channel. . conda install xeus-cling -c QuantStack -c conda-forge . After starting Jupyter Lab with jupyter lab, you should now see two additional kernels:xeus C++11 and xeus C++14. You can use either of them to use write interactive C++ programs. . Now you are ready to start playing with C++ interactively. . Thanks for reading! .",
            "url": "https://blog.andersonbanihirwe.dev/2018/01/20/play-interactively-with-cpp-getting-started-with-xeus-cling.html",
            "relUrl": "/2018/01/20/play-interactively-with-cpp-getting-started-with-xeus-cling.html",
            "date": " ‚Ä¢ Jan 20, 2018"
        }
        
    
  
    
        ,"post12": {
            "title": "Why Parallel Computing?",
            "content": "For many years we‚Äôve enjoyed the fruits of ever faster processors. However, because of physical limitations the rate of performance improvement in conventional processors is decreasing. In order to increase the power of processors, chipmakers have turned to multicore integrated circuits, that is, integrated circuits with multiple conventional processors on a single chip. . . From 1986 to 2002 the performance of microprocessors increased, on average, $50 %$ per year [[1]]. Since 2002, however, single processor performance improvement has slowed to about $20 %$ per year. This difference is dramatic: at $50%$ per year, performance will increase by almost a factor of 60 in 10 years, while at $20 %$, it will only increased by about a factor of 6. . By 2005, most of the major manufacturers of microprocessors had decided that the road to rapidly increasing performance lay in the direction of parallelism. Rather than trying to continue to develop ever-faster monolithic processors, manufacturers started putting multiple complete processors on a single integrated circuit. . All of this raises a number of questions: . Why do we care? Aren&#39;t single processor systems fast enough? After all, $20 %$ per year is still a pretty significant performance improvement. . | Why can&#39;t microprocessor manufacturers continue to develop much faster single processor systems? Why build parallel systems? Why build systems with multiple processors? . | Why can&#39;t we write programs that will automatically convert serial programs into parallel programs, that is, programs that take advantage of the presence of multiple processors? . | . Let&#39;s take a brief look at each of these questions. . Why we need ever-increasing performance . The vast increases in computational power that we have been enjoying for decades now have been at the heart of many of the most dramatic advances in fields as diverse as science, the Internet, and entertainment. The following are a few examples: . Climate modeling: In order to better understand climate change, we need far more accurate computer models, models that include interactions between the atmosphere, the oceans, solid land, the ice caps at the poles. We also need to be able to make detailed studies of how various interventions might affect the global climate. | . . Protein folding: It&#39;s believed that misfolded proteins may be involved in diseases such as Huntington&#39;s disease, Parkinson&#39;s disease, Alzheimer&#39;s disease, but our ability to study configurations of complex molecules such as proteins is severely limited by our current computational power. . | Drug discovery: There are many ways in which increased computational power can be used in research into new medical treatments. For example, there are many drugs that are affective in treating a relatively small fraction of those suffering from some disease. It&#39;s possible that we can devise alternative treatments by careful analysis of the genomes of the individuals for whom the known treatment is ineffective. This, however, will involve extensive computational analysis of genomes. . | . Although the video below sheds light on how supercomputers are unraveling the mystery of the human brain by delivering the most exquisitely detailed human brain models ever created, It is a wonderful example of how ever-increasing performance is a game changer. What if instead of behavioral research, scientists could use the models to study cognition, simulate and study disease, and learn from the ways of the brain‚Äîeverything from how it uses energy to how memory, representation and even consciousness itself are constructed? . . Data Analysis: We generate tremendous amounts of data. By some estimates, the quantity of data stored worldwide doubles every two years [2], but the vast majority of it is largely useless unless it is analyzed. For instance, knowing the sequence of nucleotides in human DNA is, by itself, of little use. Understanding how this sequence affects development and how it can cause disease requires extensive analysis. In addition to genomics, vast quantities of data are generated by particle colliders such as the Large Hadron Collider at CERN, medical imaging, astronomical research, and Web search engines, etc.. | . . These and a host of other problems won&#39;t be solved without vast increases in computational power. . Why we are building parallel systems . Much of the tremendous increase in single processor performance has been driven by the ever-increasing density of transistors-the electronic switches-on integrated circuits. As the size of transistors decreases, their speed can be increased, and the overall speed of the integrated circuit can be increased. However, as the speed of transistors increases, their power consumption also increases. Most of this power is dissipated as heat, and when an integrated circuit gets too hot, it becomes unreliable. Therefore, it is becoming impossible to continue to increase the speed of integrated circuits. However, the increase in transistor density can continue at least for while. . How then, can we exploit the continuing increase in transistor density? The answer is parallelism. Rather than building ever-faster, more complex, monolithic processors, the industry has decided to put multiple, relatively simple, complete processors on a single chip. Such integrated circuits are called multicore processors, and core has become synonymous with central processing unit, or CPU. . . Why we need to write parallel programs . Most programs that have been written for conventional; single-core systems cannot exploit the presence of multiple cores. We can run multiple instances of a program on a multicore system, but this is often of little help. For instance, being able to run multiple instances of our favorite game program is not really what we want - we want the program to run faster with more realistic graphics. . In order to do this, we need to either rewrite our serial programs so that they are parallel, so that they can make use of multiple cores, or write translation programs, that is, programs that will automatically convert serial programs into parallel programs. The bad news is that researchers have had very limited success writing programs that convert serial programs in languages such as C and C++ into parallel programs. . An efficient parallel implementation of a serial program may not be obtained by finding efficient parallelizations of each of its steps. Rather, the best parallelization may be obtained by stepping back and devising and entirely new algorithm. . As an example, suppose we need to compute $n$ values and add them together. This can be done with the following serial code: . total_sum = 0. for i in range(0, n): x = compute_next_value(...) total_sum += x . Now suppose we also have $p$ cores and $p$ is much smaller than $n$. Then each core can form a partial sum of approximately $ frac{n}{p}$ values: . my_sum = 0. my_first = ... my_last_i = ... for my_i in range(my_first, my_last_i): my_x = compute_next_value(...) my_sum += my_x . Here the prefix my__ indicates that each core is using its own, private variables, and each core can execute this block of code independently of the other cores. . After each core completes execution of this code, its variable my_sum will store the sum of the values computed by its calls to compute_next_value(). For instance, if there are eight cores, $n=24$, and the $24$ calls to compute_next_value() return the values . $$1, 4, 3, 9, 2, 8, 5, 1, 1, 6, 2, 7, 2, 5, 0, 4, 1, 8, 6, 5, 1, 2, 3, 9,$$ . then the values store in my_sum might be . . Here we&#39;re assuming the cores are identified by nonnegative integers in the range $0,1, dots, p-1$, where $p$ is the number of cores. . When the cores are done computing their values of my_sum, they can form a global sum by sending their results to a designated master core, which can add their results. . if (&#39;I am the master core&#39;): total_sum = my_x for each core other than myself: receive value from core total_sum += value else: send my_x to the master core . In our example, if the master core is core $0$, it would add the values $8+19+7+15+7+13+12+14=95$. . A better way to do this--especially if the number of cores is large is instead of making the master core do all the work of computing the final sum, we can pair the cores so that while core $0$ adds in the result of core $1$, core $2$ can add in the result of core $3$ and so on. Then we can repeat the process with only the even-ranked cores: $0$ adds in the result of $2$, $4$ adds in the result of $6$, and so on. Now cores divisible by $4$ repeat the process, and so on. . . This method results in an improvement of more than a factor of two. The difference becomes much more dramatic with large numbers of cores. With 1000 cores, the first method will require $999$ receives and adds, while the second will only require $10$, an improvement of almost a factor of $100$! . We might expect that software could be written so that a large number of common serial constructs could be recognized and efficiently parallelized, that is, modified so that they can use multiple cores. However, as we apply this principle to ever more complex serial programs, it becomes more and more difficult to recognize the construct, and it becomes less and less likely that we&#39;ll have a precoded, efficient parallelization. . Thus, we cannot simply continue to write serial programs, we must write parallel programs, programs that exploit the power of multiple processors. . How do we write parallel programs . There are a number of possible answers to this question, but most of them depend on the basic idea of partitioning the work to be done among the cores. There are two widely used approaches: . Task-parallelism: is the simultaneous execution on multiple cores of many different functions across the same or different datasets. | Data-parallelism (aka SIMD): is the simultaneous execution on multiple cores of the same function across the elements of a dataset. | . The first part of global sum example above would probably be considered and example of data parallelism. The data are the values computed by compute_next_value(), and each core carries out roughly the same operations on its assigned elements. . The second part of this global sum example might be considered an example of task parallelism. There are two tasks: receiving, and adding the cores&#39; partial sums, which is carried out by the master core, and giving the partial sum to the master core, which is carried out by the other cores. . In both global sum examples, the coordination involves communication: one or more cores send their current partial sums to another one. The global sum examples should also involve coordination through load balancing: even though we didn&#39;t give explicit formulas, it is clear that we want the cores all to be assigned roughly the same number of values to compute. If, for example, one core has to compute most of the values, then the other cores will finish much sooner than the heavily loaded core, and their computational power will be wasted. . A third type of coordination is synchronization. In most systems the cores are not automatically synchronized. Rather, each core works at its own pace. In this case, the problem is that we don&#39;t want the cores to race ahead and start computing their partial sums before the master is done initializing $x$ and making it available to other cores. That is, the cores need to wait before starting execution of the code. . We need to add in a point of synchronization between the initialization of $x$ and the computation of the partial sums: synchronize_cores(). . The idea here is that each core will wait in the function synchronize_cores() until all the cores have entered the function--in particular, until the master core has entered this function. . Currently, the most powerful parallel programs are written using explicit parallel constructs, that is, they are written using extensions to languages such as C and C++. The complexity of modern cores often makes it necessary to use considerable care in writing the code that will be executed by a single core. . References . [1] M. Herlihy, N. Shavit, The Art of Multiprocessor Programming, Morgan Kaufmann, Boston, 2008. . | [2] IBM, IBM InfoSphere streams v1.2.0 supports highly complex heterogeneous data analysis, IBM United States Software Announcement 210-037, February 23, 2010. Available here . | An Introduction to Parallel Programming by Peter Pacheco | .",
            "url": "https://blog.andersonbanihirwe.dev/2017/10/20/why-parallel-computing.html",
            "relUrl": "/2017/10/20/why-parallel-computing.html",
            "date": " ‚Ä¢ Oct 20, 2017"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": ". About Me . My name is Anderson Banihirwe. Currently, I am working as a software engineer in Computational &amp; Information Systems Lab (CISL) at the National Center for Atmospheric Research (NCAR) while contributing to Pangeo project‚Äôs efforts. . I contribute to and maintain several libraries within the open source scientific Python stack, particularly around improving scalability of Python tools in order to handle petabyte-scale datasets on HPC and cloud platforms. Most projects that I work on can be found on my GitHub page. . I graduated from University of Arkansas at Little Rock with a Bachelor of Science degree in Systems Engineering with a focus on Computer Systems. . About This Blog . This is a place for me to share thoughts, ideas, projects with the world. If you are reading this, I am honored for taking some time out of your life. If you have read something and wish to contact me about it, please send me an email or via Twitter @andersy005, I will be happy to discuss it with you! Do contact me if you think there‚Äôs room for collaboration. . This website is powered by fastpages 1. . . Be prepared: ‚ÄúLuck‚Äù is truly where preparation meets opportunity. ~ Professor Randy Pausch . Look at the world differently. In a world of talkers, be a thinker and a doer. ~ Destin Sandlin . Experience is what you get when you didn‚Äôt get what you wanted. And experience is often the most valuable thing you have to offer. ~ Professor Randy Pausch . . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://blog.andersonbanihirwe.dev/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Projects",
          "content": "I‚Äôve been extremely lucky to work in realms that allow me to cross institutional and geographic boundaries to collaborate on tools and research software. More generally I write open source tools for scientific computing in Python. . Intake-esm: this package makes it much easier for scientists to load cataloged Earth System Model data on an HPC system or in the cloud. . | Pooch: a Python package that manages downloading data files over HTTP and storing them in a local directory. . | xarray: a Python package for working with labeled multi-dimensional arrays. . | Dask: a Python library for parallel computing. . | . For a much more up-to-date list of my work, check out my GitHub page. .",
          "url": "https://blog.andersonbanihirwe.dev/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
      ,"page8": {
          "title": "Talks",
          "content": "I‚Äôve given talks at academic and software conferences. Some of these are recorded and available online. Here‚Äôs a mostly up to date list of all the talks I‚Äôve given. . Zarr - Cloud Native Geospatial Outreach Day 2020. September 2020 | Intake &amp; Intake-ESM ‚Äì Taking the pain out of data access and distribution - Jupyter meets the Earth: EarthCube Community Meeting. July 2020 | Intake-ESM ‚Äì Making It Easier To Consume Climate and Weather Data - ESIP Summer Meeting 2020. July 2020 | Intake-esm ‚Äì Making It Easier To Consume Climate Data - CMIP6 Hackathon 2019. Boulder, CO. | JupyterHub on HPC - Pangeo Meeting 2019. Seattle, WA. August 2019 | Intake-esm - Pangeo Meeting 2019. Seattle, WA. August 2019 | Turning HPC Systems into Interactive Data Analysis Platforms using Jupyter and Dask - SciPy 2019. Austin, TX. July 2019 | Beyond Matplotlib: Building Interactive Climate Data Visualizations with Bokeh and Friends - UCAR Software Engineering Assembly. Boulder, CO. April 2018 . | PySpark for ‚ÄúBig‚Äù Atmospheric Data Analysis - American Meteorological Society (AMS)Conference. Austin, TX. Jan 2018 . | PySpark for ‚ÄúBig‚Äù Atmospheric and Oceanic Data Analysis - National Center for Atmospheric Research. Boulder, CO. Aug 2017 | .",
          "url": "https://blog.andersonbanihirwe.dev/talks/",
          "relUrl": "/talks/",
          "date": ""
      }
      
  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://blog.andersonbanihirwe.dev/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}