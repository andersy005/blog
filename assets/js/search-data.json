{
  
    
        "post0": {
            "title": "Introducing fastpages",
            "content": ". We are very pleased to announce the immediate availability of fastpages. fastpages is a platform which allows you to create and host a blog for free, with no ads and many useful features, such as: . Create posts containing code, outputs of code (which can be interactive), formatted text, etc directly from Jupyter Notebooks; for instance see this great example post from Scott Hawley. Notebook posts support features such as: Interactive visualizations made with Altair remain interactive. | Hide or show cell input and output. | Collapsable code cells that are either open or closed by default. | Define the Title, Summary and other metadata via a special markdown cells | Ability to add links to Colab and GitHub automatically. | . | Create posts, including formatting and images, directly from Microsoft Word documents. | Create and edit Markdown posts entirely online using GitHub&#39;s built-in markdown editor. | Embed Twitter cards and YouTube videos. | Categorization of blog posts by user-supplied tags for discoverability. | ... and much more | . fastpages relies on Github pages for hosting, and Github Actions to automate the creation of your blog. The setup takes around three minutes, and does not require any technical knowledge or expertise. Due to built-in automation of fastpages, you don&#39;t have to fuss with conversion scripts. All you have to do is save your Jupyter notebook, Word document or markdown file into a specified directory and the rest happens automatically. Infact, this blog post is written in a Jupyter notebook, which you can see with the &quot;View on GitHub&quot; link above. . fast.ai have previously released a similar project called fast_template, which is even easier to set up, but does not support automatic creation of posts from Microsoft Word or Jupyter notebooks, including many of the features outlined above. . Because fastpages is more flexible and extensible, we recommend using it where possible. fast_template may be a better option for getting folks blogging who have no technical expertise at all, and will only be creating posts using Github&#39;s integrated online editor. . Setting Up Fastpages . The setup process of fastpages is automated with GitHub Actions, too! Upon creating a repo from the fastpages template, a pull request will automatically be opened (after ~ 30 seconds) configuring your blog so it can start working. The automated pull request will greet you with instructions like this: . . All you have to do is follow these instructions (in the PR you receive) and your new blogging site will be up and running! . Jupyter Notebooks &amp; Fastpages . In this post, we will cover special features that fastpages provides has for Jupyter notebooks. You can also write your blog posts with Word documents or markdown in fastpages, which contain many, but not all the same features. . Options via FrontMatter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . All of the above settings are enabled in this post, so you can see what they look like! . the summary field (preceeded by &gt;) will be displayed under your title, and will also be used by social media to display as the description of your page. | toc: setting this to true will automatically generate a table of contents | badges: setting this to true will display Google Colab and GitHub links on your blog post. | comments: setting this to true will enable comments. See these instructions for more details. | author this will display the authors names. | categories will allow your post to be categorized on a &quot;Tags&quot; page, where readers can browse your post by categories. | . Markdown front matter is formatted similarly to notebooks. The differences between the two can be viewed on the fastpages README. . Code Folding . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . If you want to completely hide cells (not just collapse them), read these instructions. . Interactive Charts With Altair . Interactive visualizations made with Altair remain interactive! . We leave this below cell unhidden so you can enjoy a preview of syntax highlighting in fastpages, which uses the Dracula theme. . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;IMDB_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget IMDB_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | 6.1 | . 1 First Love, Last Rites | 10876.0 | 300000.0 | 6.9 | . 2 I Married a Strange Person | 203134.0 | 250000.0 | 6.8 | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | NaN | . 4 Slam | 1087521.0 | 1000000.0 | 3.4 | . Other Feautures . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Images w/Captions . You can include markdown images with captions like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Of course, the caption is optional. . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . More Examples . This tutorial contains more examples of what you can do with notebooks. . How fastpages Converts Notebooks to Blog Posts . fastpages uses nbdev to power the conversion process of Jupyter Notebooks to blog posts. When you save a notebook into the /_notebooks folder of your repository, GitHub Actions applies nbdev against those notebooks automatically. The same process occurs when you save Word documents or markdown files into the _word or _posts directory, respectively. . We will discuss how GitHub Actions work in a follow up blog post. . Resources &amp; Next Steps . We highly encourage you to start blogging with fastpages! Some resources that may be helpful: . fastpages repo - this is where you can go to create your own fastpages blog! | Fastai forums - nbdev &amp; blogging category. You can ask questions about fastpages here, as well as suggest new features. | nbdev: this project powers the conversion of Jupyter notebooks to blog posts. | . If you end up writing a blog post using fastpages, please let us know on Twitter: @jeremyphoward, @HamelHusain. .",
            "url": "https://andersonbanihirwe.dev/fastpages/jupyter/2020/02/21/introducing-fastpages.html",
            "relUrl": "/fastpages/jupyter/2020/02/21/introducing-fastpages.html",
            "date": " • Feb 21, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://andersonbanihirwe.dev/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "A Knitting Weekend",
            "content": "For the last few months, I&#39;ve become a huge fan of the quiet weekend, the weekend when no plan is the plan, and you are in no hurry at all. I woke up on Saturday morning, and I decided that this weekend was going to be a quiet one. After watching a late morning football game from the English Premier League -- yup, I refuse to call it soccer 😀 -- I decided to write some code-nothing big, just a few lines of codes to improve my experience with Dash, a tool that I&#39;ve been using for a few months. Dash is a MacOS application that offers offline documentation for hundreds of programming languages, fraweworks and libraries. Dash is awesome for offline programming when using spotty WiFi or on long-haul flights. While it is free as in beer, it&#39;s one of the programs that are worth paying for. . The Problem . I&#39;ve been using Dash for quite some time and have been really satisfied with the preset selection of documentation sets. At some point, I started running into scenarios when I was using frameworks/libraries whose documentations aren&#39;t available for Dash. So, my objective was to generate documentation sets for these libraries myself. . The Solution: Doc2dash and Sphinx To The Rescue . doc2dash is a great tool for converting HTML documentation to Dash&#39;s format. doc2dash takes the output of Sphinx documentation and converts it to a Dash documentation package. . Show Me The Code! . In Just a few steps, I was able to build a docset for Zarr using dash2doc: . Create new conda environment and install dependencies | . $ conda create -n dash-docs -c conda-forge doc2dash sphinx-rtd-theme numpydoc sphinx-issues $ conda activate dash-docs . Pull down the project form GitHub and build the documentation | . $ git clone https://github.com/zarr-developers/zarr-python.git $ cd zarr-python $ python -m pip install -e . $ cd docs $ make html . Finally, run doc2dash with a few commands: | . $ doc2dash --name zarr --index-page index.html --enable-js --add-to-dash _build/html . And with that I had a shiny new Zarr docset in Dash: . . Once I was confident that I understood how to do this for one project, I decided to automate this process for a bunch of other projects. By dinner time (on Saturday) I felt like I was on the right track. After a few more hours on Sunday and Monday, I had a GitHub repository with continuous integration via CircleCI up and running, and had a dozen docsets generated: . . . Was It Worth Fifteen or Twenty Hours of My Life? . I think so: I took pleasure in doing something that I knew how to do and in creating something that would fit my needs perfectly. . Sitting in a chair in front a computer screen while listening to MIX 100 Denver Radio station 😀, I was reminded of the intriguing parallels between knitting and programming: . &quot;Knitting is Coding, and Yarn is a Programmable material.&quot; . On quiet weekends like this I write code for fun, and Visual Studio Code &amp; Python are my yarn and needles. . . Note: Sometimes a quiet weekend is an absolute gift! .",
            "url": "https://andersonbanihirwe.dev/blog/2020/01/21/a-knitting-weekend.html",
            "relUrl": "/blog/2020/01/21/a-knitting-weekend.html",
            "date": " • Jan 21, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://andersonbanihirwe.dev/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Microsoft Word Example Post",
            "content": "When writing a blog post with Microsoft Word – the filename becomes the title. In this case the file name is “2020-01-01-Microsoft-Word-Example-Post.docx”. . There is minimal support for Word documents in fastpages compared to Jupyter notebooks. Some known limitations: . alt text in Word documents are not yet supported by fastpages, and will break links to images. . | You can only specify front matter for Word documents globally. See the README for more details. . | . For greater control over the content produced from Word documents, you will need to convert Word to markdown files manually. You can follow the steps in this blog post, which walk you through how to use pandoc to do the conversion. Note: If you wish to customize your Word generated blog post in markdown, make sure you delete your Word document from the _word directory so your markdown file doesn’t get overwritten! . If your primary method of writing blog posts is Word documents, and you plan on always manually editing Word generated markdown files, you are probably better off using fast_template instead of fastpages. . The material below is a reproduction of this blog post, and serves as an illustrative example. . Maintaining a healthy open source project can entail a huge amount of toil. Popular projects often have orders of magnitude more users and episodic contributors opening issues and PRs than core maintainers capable of handling these issues. . Consider this graphic prepared by the NumFOCUS foundation showing the number of maintainers for three widely used scientific computing projects: . . We can see that across these three projects, there is a very low ratio maintainers to users. Fixing this problem is not an easy task and likely requires innovative solutions to address the economics as well as tools. . Due to its recent momentum and popularity, Kubeflow suffers from a similar fate as illustrated by the growth of new issues opened: . . Source: “TensorFlow World 2019, Automating Your Developer Workflow With ML” . Coincidentally, while building out end to end machine learning examples for Kubeflow, we built two examples using publicly available GitHub data: GitHub Issue Summarization and Code Search. While these tutorials were useful for demonstrating components of Kubeflow, we realized that we could take this a step further and build concrete data products that reduce toil for maintainers. . This is why we started the project kubeflow/code-intelligence, with the goals of increasing project velocity and health using data driven tools. Below are two projects we are currently experimenting with : . Issue Label Bot: This is a bot that automatically labels GitHub issues using Machine Learning. This bot is a GitHub App that was originally built for Kubeflow but is now also used by several large open source projects. The current version of this bot only applies a very limited set of labels, however we are currently A/B testing new models that allow personalized labels. Here is a blog post discussing this project in more detail. . | Issue Triage GitHub Action: to compliment the Issue Label Bot, we created a GitHub Action that automatically adds / removes Issues to the Kubeflow project board tracking issues needing triage. . | Together these projects allow us to reduce the toil of triaging issues. The GitHub Action makes it much easier for the Kubeflow maintainers to track issues needing triage. With the label bot we have taken the first steps in using ML to replace human intervention. We plan on using features extracted by ML to automate more steps in the triage process to further reduce toil. . Building Solutions with GitHub Actions . One of the premises of Kubeflow is that a barrier to building data driven, ML powered solutions is getting models into production and integrated into a solution. In the case of building models to improve OSS project health, that often means integrating with GitHub where the project is hosted. . We are really excited by GitHub’s newly released feature GitHub Actions because we think it will make integrating ML with GitHub much easier. . For simple scripts, like the issue triage script, GitHub actions make it easy to automate executing the script in response to GitHub events without having to build and host a GitHub app. . To automate adding/removing issues needing triage to a Kanban board we wrote a simple python script that interfaces with GitHub’s GraphQL API to modify issues. . As we continue to iterate on ML Models to further reduce toil, GitHub Actions will make it easy to leverage Kubeflow to put our models into production faster. A number of prebuilt GitHub Actions make it easy to create Kubernetes resources in response to GitHub events. For example, we have created GitHub Actions to launch Argo Workflows. This means once we have a Kubernetes job or workflow to perform inference we can easily integrate the model with GitHub and have the full power of Kubeflow and Kubernetes (eg. GPUs). We expect this will allow us to iterate much faster compared to building and maintaining GitHub Apps. . Call To Action . We have a lot more work to do in order to achieve our goal of reducing the amount of toil involved in maintaining OSS projects. If your interested in helping out here’s a couple of issues to get started: . Help us create reports that pull and visualize key performance indicators (KPI). https://github.com/kubeflow/code-intelligence/issues/71 . We have defined our KPI here: issue #19 | . | Combine repo specific and non-repo specific label predictions: https://github.com/kubeflow/code-intelligence/issues/70 . | . In addition to the aforementioned issues we welcome contributions for these other issues in our repo. .",
            "url": "https://andersonbanihirwe.dev/2020/01/01/Microsoft-Word-Example-Post.html",
            "relUrl": "/2020/01/01/Microsoft-Word-Example-Post.html",
            "date": " • Jan 1, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Reflecting on SciPy 2019",
            "content": "It’s been a week since SciPy 2019 conference came to a close. I had the pleasure of speaking at SciPy 2019 about interactive supercomputing with Dask and Jupyter. I am so thankful for the chance to speak in the Earth, Ocean, Geo and Atmospheric Science track! . Here’s a recording of my talk: . As I look back at my first SciPy, I can say that it was the most productive conference that I’ve been to so far. So, I wanted to take this opportunity to summarize my key takeaways. . 1. The Awesomeness of SciPy Community . The biggest highlight of SciPy for me was the community. What an authentic, super-talented, enthusiastic and welcoming community! I met and interacted with a lot of amazing people that I only knew via Github prior the conference :). The community is really what makes SciPy such a special conference. . 2. Invisible Work in Open Source Software Projects . Because so much of the work in OSS is tracked on public platforms, it is easy to forget about the work that takes place outside of them. . This quote by Stuart really hit home. For the last few months, I’ve become a core maintainer of a few projects, and one of the things I’ve learned so far as a user, a contributor to, and core maintainer of an open source project is that it’s really important to recognize that the github repository does not fully represent a project. There’s quite a lot of crucial, untracked work that goes on in the background such as: . . You can watch the full key note here. . 3. Tell People When You Appreciate Their Work . Another key lessons I learned is from Carol Willing’s keynote is that research shows that meeting someone who has befitted from your work can double your effort and triple your productivity: . . 4. We Should Think About Open Source Standards . Matthew Rocklin gave a great talk on refactoring the SciPy ecosystem for heterogeneous computing. . My takeaway from his talk is that . The Python scientific community fractures when we develop competing packages that are not compatible with each other and thrives when we develop tools based on common standards. . As software developers, we should try to follow already existing standards when we can, because standards level the playing field in so many ways: . new technologies can quickly compete | new developers can quickly engage | incentivize support (e.g. GitHub renders .ipynb since Jupyter notebooks are built on a widely used standard) | . 5. Jupyter Notebooks Come Alive with Jupyter Widgets . One of my favorite SciPy 2019 talks is Martin and Maarten’s talk on Dashboarding with Jupyter Notebooks, Voilà and Widgets. It is really fun and full of cool stuffs. . I am looking forward to making my own widgets and dashboards! And exclaiming voilà! after I deploy them! . 6. Maslow’s Hierarchy of Software Needs . Stan’s talk on How to Accelerate an Existing Codebase with Numba is also among my favorites. The Key lesson from this talk is the following: . Be honest with yourself about where your project is in this hierarchy. If your program doesn’t run, there’s no point in making it faster. . . 7. Final Thoughts . I left SciPy 2019 feeling: . very thankful that so many amazing people are able to come out and collaborate on the projects that are so vital to the Python scientific ecocystem! | grateful to be part of this community. | inspired to keep contributing to this community. | so excited about the technology, the science, and the people. | . Thank you so much to the SciPy Conference organizers for putting on a great conference. .",
            "url": "https://andersonbanihirwe.dev/blog/2019/07/21/scipy-2019.html",
            "relUrl": "/blog/2019/07/21/scipy-2019.html",
            "date": " • Jul 21, 2019"
        }
        
    
  
    
        ,"post6": {
            "title": "Summary of TVM -- End-to-End Optimization Stack for Deep Learning",
            "content": "Abstract . Paper link: https://arxiv.org/abs/1802.04799 . Scalable frameworks, such as TensorFlow, MXNet, Caffe, and PyTorch are optimized for a narrow range of serve-class GPUs. | Deploying workloads to other platforms such as mobile phones, IoT, and specialized accelarators(FPGAs, ASICs) requires laborious manual effort. | TVM is an end-to-end optimization stack that exposes: . graph-level | operator-level optimizations | . &gt; to provide performance portability to deep learning workloads across diverse hardware back-ends. . | . Introduction . The number and diversity of specialized deep learning (DL) accelerators pose an adoption challenge . They introduce new hardware abstractions that modern compilers and frameworks are ill-equipped to deal with. | . | Providing support in various DL frameworks for diverse hardware back-ends in the present ad-hoc fashion is unsustainable. . | Hardware targets significantly diverge in terms of memory organization, compute, etc.. . | . . The Goal: easily deploy DL workloads to all kinds of hardware targets, including embedded devives, GPUs, FPGAs, ASCIs (e.g, the TPU). . | Current DL frameworks rely on a computational graph intermediate representation to implement optimizations such as: . auto differentiation | dynamic memory management | . | Graph-level optimizations are often too high-level to handle hardware back-end-specific operator transformations. . | Current operator-level libraries that DL frameworks rely on are: . too rigid | specialized | . &gt; to be easily ported across hardware devices . | To address these weaknesses, we need a compiler framework that can expose optimization opportunities across both . graph-level and | operator-level | . &gt; to deliver competitive performance across hardware back-ends. . | . Four fundamental challenges at the computation graph level and tensor operator level . High-level dataflow rewriting: . Different hardware devices may have vastly different memory hierarchies. . | Enabling strategies to fuse operators and optimize data layouts are crucial for optimizing memory access. . | . | Memory reuse across threads: . Modern GPUs and specialized accelerators ahve memory that can be shared across compute cores. | Traditional shared-nothing nested parallel model is no longer optimal. | Cooperation among threads on shared memory loaded is required for optimized kernels. | . | Tensorized compute intrinsics: . The latest hardware provides new instructions that go beyond vector operations like the GEMM operator in TPU or the tensor core in NVIDIA&#39;s Volta. | Consequently, the scheduling procedure must break computation into tensor arithmetic intrinsics instead of scalar or vector code. | . | Latency Hiding . Traditional architectures with simultaneous multithreading and automatically managed caches implicitly hide latency in modern CPUs/GPUs. | Specialized accelerator designs favor learner control and offload most of the scheduling complexity to the compiler stack. | Still, scheduling must be peformed carefully to hide memory access latency. | . | TVM: An End-to-End Optimization Stack . An end-to-end optimizing compiler stack to lower and fine-tune DL workloads to diverse hardware back-ends. | Designed to separate: the algorithm description | schedule | hardware interface | . | This separation enables support for novel specialized accelerators and their corresponding new intrinsics. | TVM presents two optimization layers: a computation graph optimization layer to address: High-level dataflow rewriting | . | a tensor optimization layer with new schedule primitives to address: memory reuse across threads | tensorized compute intrinsics | latency hiding | . | . | . Optimizing Computational Graphs . Computational Graph . Computational graphs are a common way to represent programs in DL frameworks. | They provide a global view on computation tasks, yet avoid specifying how each computation task needs to be implemented. | . Operator Fusion . An optimization that can greatly reduce execution time, particulary in GPUs and specialized accelerators. | The idea is to combine multiple operators together into a single kernel without saving the intermediate results back into global memory | . . Four categories of graph operators: . Injective (one-to-one map) | Reduction | Complex-out-fusable (can fuse element-wise map to output) | Opaque (cannot be fused) | . . Data Layout Transformation . Tensor operations are the basic operators of computational graphs | They can have divergent layout requirements across different operations | Optimizing data layout starts with specifying the preferred data layout of each operator given the constraints dictating their implementation in hardware. | . . Limitations of Graph-Level Optimizations . They are only as effective as what the operator library provides. | Currently, the few DL frameworks that support operator fusion require the operator library to provide an implementation of the fused patterns. With more network operators introduced on a regular basis, this approach is no longer sustainable when targeting an increasing number of hardware back-ends. | . | It is not feasible to handcraft operator kernels for this massive space of back-end specific operators TVM provides a code-generation approach that can generate tensor operators. | . | . Optimizing Tensor Operations . Tensor Expression Language . TVM introduces a dataflow tensor expression language to support automatic code generation. | Unlike high-level computation graph languages, where the implementation of tensor operations is opaque, each operation is described in an index formula expression language. | . . TVM tensor expression language supports common arithmetic and math operations found in common language like C. | TVM explicitly introduces a commutative reduction operator to easily schedule commutative reductions across multiple threads. | TVM further introduces a high-order scan operator that can combine basic compute operators to form recurrent computations over time. | . Schedule Space . Given a tensor expression, it is challenging to create high-performance implementations for each hardware back-end. | Each optimized low-level program is the result of different combinations of scheduling strategies, imposing a large burden on the kernel writer. | TVM adopts the principle of decoupling compute descriptions from schedule optimizations. | Schedules are the specific rules that lower compute descriptions down to back-end-optimized implementations. | . . . Nested Parallelism with Cooperation . Parallel programming is key to improving the efficiency of compute intensive kernels in deep learning workloads. | Modern GPUs offer massive parallelism . &gt; Requiring TVM to bake parallel programming models into schedule transformations . | Most existing solutions adopt a parallel programming model referred to as nested parallel programs, which is a form of fork-join parallelism. . | TVM uses a parallel schedule primitive to parallelize a data parallel task Each parallel task can be further recursively subdivided into subtasks to exploit the multi-level thread hierarchy on the target architecture (e.g, thread groups in GPU) | . | This model is called shared-nothing nested parallelism . One working thread cannot look at the data of its sibling within the same parallel computation stage. | Interactions between sibling threads happen at the join stage, when the subtasks are done and the next stage can consume the data produced by the previous stage. | This programming model does not enable threads to cooperate with each other in order to perform collective task within the same parallel stage. | . | A better alternative to the shared-nothing approach is to fetch data cooperatively across threads . This pattern is well known in GPU programming using languages like CUDA, OpenCL and Metal. | It has not been implemented into a schedule primitive. | . | TVM introduces the concept of memory scopes to the schedule space, so that a stage can be marked as shared. Without memory scopes, automatic scope inference will mark the relevant stage as thread-local. | Memory scopes are useful to GPUs. | Memory scopes allow us to tag special memory buffers and create special lowering rules when targeting specialized deep learning accelerators. | . | . . Tensorization: Generalizing the Hardware Interface . Tensorization problem is analogous to the vectorization problem for SIMD architectures. | Tensorization differs significantly from vectorization The inputs to the tensor compute primitives are multi-dimensional, with fixed or variable lengths, and dictate different data layouts. | Cannot resort to a fixed set of primitives, as new DL accelerators are emerging with their own flavors of tensor instructions. | . | To solve this challenge, TVM separates the hardware interface from the schedule: TVM introduces a tensor intrinsic declaration mechanism | TVM uses the tensor expression language to declare the behavior of each new hardware intrinsic, as well as the lowering rule associated to it. | TVM introduces a tensorize schedule primitive to replace a unit of computation with the corresponding tensor intrinsics. | The compiler matches the computation pattern with a hardware declaration, and lowers it to the corresping hardware intrinsic. | . | . Compiler Support for Latency Hiding . Latency Hiding: refers to the process of overlapping memory operations with computation to maximize memory and compute utilization. | It requires different different strategies depending on the hardware back-end that is being targeted. | On CPUs, memory latency hiding is achieved implicitly with simultaneous multithreading or hardware prefetching techniques. | GPUs rely on rapid context switching of many wraps of threads to maximize the utilization of functional units. | TVM provides a virtual threading schedule primitive that lets the programmer specify a high-level data parallel program that TVM automatically lowers to a low-level explicit data dependence program. | . Code Generation and Runtime Support . Code Generation . For a specific tuple of data-flow declaration, axis relation hyper-graph, and schedule tree, TVM can generate lowered code by: iteratively traversing the schedule tree | inferring the dependent bounds of the input tensors (using the axis relation hyergraph) | generating the loop nest in the low-level code | . | The code is lowered to an in-memory representation of an imperative C style loop program. | TVM reuses a variant of Halide&#39;s the loop program data structure in this process. | TVM reuses passes from Halide for common lowering primitives like storage flattening and unrolling, and add GPU/accelerator-specific transformations such as: synchronization point detection | virtual thread injection | module generation | . | . | Finally, the loop program is transformed into LLVM or CUDA/Metal/OpenCL source code. | . Runtime Support . For GPU programs, TVM builds the host and device modules separately and provide a runtime module system that launch kernels using corresponding driver APIs. | . Remote Deployment Profiling . TVM includes infrastructure to make profiling and autotuning easier on embedded devices. | Traditionally, targeting an embedded device for tuning requires: . cross-compiling on the host side, | copying to the target device, | and timing the execution | . | TVM provides remote function call support. Through the RPC interface: . TVM compiles the program on a host compiler | it uploads to remote embedded devices | it runs the funcion remotely, | and it accesses the results in the same script on the host. | . | . . Conclusion . TVM provides an end-to-end stack to solve fundamental optimization challenges across a diverse set of hardware back-ends. | TVM can encourage more studies of programming languages, compilation, and open new opportunities for hardware co-design techniques for deep learning systems. | .",
            "url": "https://andersonbanihirwe.dev/blog/paper/2018/05/10/TVM-paper-summary.html",
            "relUrl": "/blog/paper/2018/05/10/TVM-paper-summary.html",
            "date": " • May 10, 2018"
        }
        
    
  
    
        ,"post7": {
            "title": "Play interactively with C++: STL Sequence Containers",
            "content": "The Standard Template Library (STL) is a programmer&#39;s dream. It offers efficient ways to: . store | access | manipulate | view data | . and is designed for maximum extensibility. Once a programmer has gotten over the initial syntax hurdles, they quickly learn to appreciate the STL&#39;s sheer power and flexibility. . Overview of STL . Containers: a collection of container classes. For example: . map: an associative collection of key/value pairs | vector: a growing list of elements. | . | Iterators: objects that view and modify ranges of stored data. Each STL container exports iterators. Iterators have a common interface, allowing the programmer to write code that operates on data stored in arbitrary containers. . | Algorithms: functions that operate over ranges of data specified by iterators. . | Adapters: objects which transform an object from one form into another. For instance: . stack adapter transforms a regular vector or list into a LIFO (Last In First Out) container. | istream_iterator transforms a standard C++ stream inot an STL iterator. | . | Functions: facilities for creating and modifying functions at runtime. . | Allocators: allow clients of the container classes to customize how memory is allocated and deallocated, either for diagnostic or performance reasons. . | . Vectors . A std::vector is an object that represents a sequence of elements. The elements in a vector are indexed, meaning that they can have a well-defined position in the sequence. . #include &lt;iostream&gt; #include &lt;vector&gt; // Necessary to use vector #include &lt;string&gt; #include &lt;sstream&gt; . // Create a vector containing integers std::vector&lt;int&gt; v = {7, 10, 15, 8, 98, 0}; . Add two more integers to vector using push_back() . v.push_back(45); v.push_back(56); . Helper function to iterate and print values of vector . void PrintVector(std::vector&lt;int&gt;&amp; v){ for(int n : v){ std::cout &lt;&lt; n &lt;&lt; &#39; &#39;; } std::cout &lt;&lt; std::endl; } . PrintVector(v) . 7 10 15 8 98 0 45 56 . The storage of the vector is handled automatically, being expanded and contracted as needed. Vectors usually occupy more space than static arrays, because more memory is allocated to handle future growth. This way a vector does not need to reallocate each time an element is inserted, but only when the additional memory is exhausted. . Grow the vector, setting new elements to 0 . v.resize(10); PrintVector(v) . 7 10 15 8 98 0 45 56 0 0 . The total amount of allocated memory can be queried using capacity() function. Extra memory can be returned to the system via a call to shrink_to_fit() . capacity(): returns the number of element that can be held in currently allocated storage . v.capacity() . 12 . shrink_to_fit(): reduces memory usage by freeing unused memory . v.shrink_to_fit() . v.capacity() . 10 . resize(): changes the number of elements stored. E.g; Grow the vector, setting new elements to 100** . v.resize(13, 100); PrintVector(v); . 7 10 15 8 98 0 45 56 0 0 100 100 100 . v.capacity() . 20 . pop_back(): removes the last element . v.pop_back(); PrintVector(v); . 7 10 15 8 98 0 45 56 0 0 100 100 . NOTE: for detailed information about std::vector, check out the C++ reference. . deque . There are several aspects of the vector that can be troublesome in certain applications. In particular, the vector is only designed to grow in one direction; calling push_back() inserts elements at the end of the vector, and resize() always appends elements to the end. . A std:deque is an indexed sequence container that allows fast insertion and deletion at both its beginning and its end. As opposed to std:vector, the elements of a deque are not stored contiguously: typical implementations use a sequence of individually allocated fixed-size arrays, with additional bookkeeping, which means indexed access to deque must perform two pointer dereferences, compared to vector&#39;s indexed access which performs only one. . #include &lt;iostream&gt; #include&lt;deque&gt; . Create a deque (pronounced &quot;deck&quot;) containing integers . std::deque&lt;int&gt; d = {45, 48, 40, 79}; . void PrintDeque(std::deque&lt;int&gt;&amp; d){ for(int n : d){ std::cout &lt;&lt; n &lt;&lt; &#39; &#39;; } std::cout &lt;&lt; std::endl; } . PrintDeque(d); . 45 48 40 79 . Add an integer to the beginning and end of the queue . d.push_front(13); d.push_back(25); . PrintDeque(d); . 13 45 48 40 79 25 . A deque can do everything a vector can do and also unlike a vector, it is possible (and fast) to push_front and pop_front . NOTE: for detailed information about std::deque, check out the C++ reference or just type ?std::queue in a notebook cell to get live documentation. . Container Adapters . How can we implement stack and queue using the containers we have? . Stack . Stack is a type of container adapters with LIFO (Last In First Out) or FILO (First in, Last Out) data structure, where a new element is added at one end and (top) an element is removed from that end only. . Stack: just limit the functionality of a vector/deque to only allow push_back and pop_back. . The functions associated with stack are: . empty() -- Returns whether the stack is empty | size() -- Returns the size of the stack | top() -- Returns a reference to the top most element of the stack | push(g) -- Adds the element &#39;g&#39; at the top of the stack | pop() -- Deletes the top most element of the stack | . #include &lt;stack&gt; . Create a stack of integers . std::stack&lt;int&gt; s; . Helper function to show the stack . void ShowStack(std::stack&lt;int&gt; mystack){ std::stack&lt;int&gt; s = mystack; while (!s.empty()){ std::cout &lt;&lt; &#39; t&#39; &lt;&lt; s.top(); s.pop(); } std::cout &lt;&lt; std::endl; } . ShowStack(s) . . Push some elements onto the stack . s.push(10); s.push(30); s.push(20); s.push(5); . ShowStack(s) . 5 20 30 10 . Get stack size . s.size() . 4 . Get the top of the stack . s.top() . 5 . Pop top of the stack . s.pop(); . ShowStack(s) . 20 30 10 . Queue . Queue is a type of container adaptors which operate in a first in first out (FIFO) type of arrangement. Elements are inserted at the back (end) and are deleted from the front. . The functions supported by queue are: . empty() -- Returns whether the queue is empty | size() -- Returns the size of the queue | front() -- Returns a reference to the first element of the queue. | back() -- Returns a reference to the last element of the queue | push(g) -- Adds the element g at the end of the queue | pop() -- Deletes the first element of the queue. | . #include &lt;queue&gt; . Create an empty queue . std::queue&lt;int&gt; q; . Helper function to show content of the queue . void ShowQueue(std::queue&lt;int&gt; myqueue){ std::queue&lt;int&gt; q = myqueue; while (!q.empty()){ std::cout &lt;&lt; &#39; t&#39; &lt;&lt; q.front(); q.pop(); } std::cout &lt;&lt; std::endl; } . ShowQueue(q) . . Add some elements to the queue . q.push(10); q.push(20); . ShowQueue(q) . 10 20 . Get queue size . q.size() . 2 . Get front of the queue . q.front() . 10 . Get back of the queue . q.back() . 20 . Pop front of the queue . q.pop() . ShowQueue(q) . 20 .",
            "url": "https://andersonbanihirwe.dev/blog/cpp/2018/01/23/play-interactively-with-cpp-sequence-containers.html",
            "relUrl": "/blog/cpp/2018/01/23/play-interactively-with-cpp-sequence-containers.html",
            "date": " • Jan 23, 2018"
        }
        
    
  
    
        ,"post8": {
            "title": "Play interactively with C++: Streams",
            "content": "A stream is an abstraction for input/output. You can think of it as a source (input) or destination (output) of characters of indefinite length. . How do you write data to a file in C++? . C++ provides a header filed called &lt;fstream&gt; (file stream) that exports the ifstream and ofstream types, streams that perform file I/O. Input/output stream class to operate on files. . Objects of this class maintain a filebuf object as their internal stream buffer, which performs input/output operations on the file they are associated with (if any). . #include &lt;fstream&gt; // std::fstream . std::fstream fs; fs.open (&quot;test.txt&quot;, std::fstream::in | std::fstream::out | std::fstream::app); fs &lt;&lt; &quot;More lorem ipsum....&quot;; // write to the created file fs.close(); // Close the stream . !cd iofiles &amp;&amp; ls &amp;&amp; cat test.txt . fs.open (&quot;test.txt&quot;, std::fstream::in | std::fstream::out | std::fstream::app); // Re-open the stream fs &lt;&lt; &quot;Last but not the least lorem ipsum!!&quot;; // Write some more to the file fs.close(); // Close the stream . !cat test.txt . More lorem ipsum....Last but not the least lorem ipsum!! . Stream Manipulators . What does the setw manipulator do? . setw(n) sets the minimum width of the input for the next stream operation. If the data doesn&#39;t meet the minimum field requirement, it is padded with the default fill character until it is proper size. . #include &lt;iostream&gt; . std::cout &lt;&lt; &quot;Output: &quot; &lt;&lt; 10 &lt;&lt; std::endl; . Output: 10 . std::cout &lt;&lt; &quot;Output: &quot; &lt;&lt; std::setw(5) &lt;&lt; 10 &lt;&lt; std::endl; . Output: 10 . What does the boolalpha manipulator do? . boolalpha determines whether or not the stream should output boolean values as 1 and 0 or as &quot;true&quot; and &quot;false&quot;. The opposite manipulator is noboolalpha, which reverses this behaviour. . std::cout &lt;&lt; true &lt;&lt; std::endl; . 1 . std::cout &lt;&lt; std::boolalpha&lt;&lt; true &lt;&lt; std::endl; . true . What do hex, dec, oct manipulator do? . They set the radix on the stream to either octal (base 8), decimal (base 10), or hexadecimal (base 16). This can be used either to format output or change the base for input. . std::cout &lt;&lt; 10 &lt;&lt; std::endl; . 10 . std::cout &lt;&lt; std::dec &lt;&lt; 10 &lt;&lt; std::endl; . 10 . std::cout &lt;&lt; std::oct &lt;&lt; 10 &lt;&lt; std::endl; . 12 . std::cout &lt;&lt; std::hex &lt;&lt; 10 &lt;&lt; std::endl; . a . What is stream failure? How do you check for it? . Because stream operations often involve transforming data from one form into another, stream operations are not always guaranteed to succeed. . Let&#39;s use the previously created test.txt file to demonstrate stream failure. . For the following program: . If the file test.txt contains NUM_INTS consecutive integer values, then this code will work correctly. | If we try to read stream data of one type into a variable of another type, rather than crashing the program or filling the variable with garbage data, the stream fails by entering an error state and the value of the variable will not change. Once the stream is in this error state, any subsequent read or write operations will automatically and silently fail which can be a serious problem . | . To check if a stream is in an erroneous state, we use the .fail() member function. . fs.open (&quot;test.txt&quot;, std::fstream::in | std::fstream::out | std::fstream::app); . int NUM_INTS = 5; . for(int i = 0; i &lt; NUM_INTS; i++){ int value; fs &gt;&gt; value; /*..... Do something Here ....*/ if(fs.fail()) break; std::cout &lt;&lt; value &lt;&lt; std::endl; } . As you can see, when the program realizes that the file contains text and not integers, it breaks out of the loop without doing any subsequent operation. . What is a stringstream? . In C++, you can&#39;t add numbers to strings and when you can, it&#39;s almost certainly won&#39;t do what you expected. . One solution to this problem is to use another kind of stream object known as a stringstream, exported by sstreamheader. Like console streams and file streams, stringstreams are stream objects. Instead of reading or writing data to an external source, stringstreams store data in temporary string buffers. . #include &lt;sstream&gt; // std::stringstream, std::stringbuf . std::stringstream ss; ss &lt;&lt; &quot;Hello World! It&#39;s &quot; &lt;&lt; 135 &lt;&lt; std::endl; . Once you have put data into a stringstream, you can retrieve the string you have created using the .str() member function. . std::string s = ss.str(); std:: cout &lt;&lt; s &lt;&lt; std::endl; . Hello World! It&#39;s 135 . stringstreams are an example of an iostream, a stream that can perform both input and output, You can both insert data inot a stringstream to convert the data to string and extract data from a stringstream to convert string data into a different format. For example: . std::stringstream myConverter; int myInt; std::string myString; double myDouble; . std::cout &lt;&lt; myInt &lt;&lt; std::endl; std::cout &lt;&lt; myString &lt;&lt; std::endl; std::cout &lt;&lt; myDouble &lt;&lt; std::endl; . 0 0 . myConverter &lt;&lt; &quot;5 World 3.14&quot;; // Insert string data . @0x10d521080 . myConverter &gt;&gt; myInt &gt;&gt; myString &gt;&gt; myDouble; // Extract mixed data . @0x10d521070 . std::cout &lt;&lt; myInt &lt;&lt; std::endl; std::cout &lt;&lt; myString &lt;&lt; std::endl; std::cout &lt;&lt; myDouble &lt;&lt; std::endl; . 5 World 3.14 . Write a function that converts an int into a string . std::string intToStr(int myInt){ std::stringstream ss; ss &lt;&lt; myInt; std::string s = ss.str(); return s; } . std::string str = intToStr(5); std::cout &lt;&lt; str &lt;&lt; std::endl; . 5 . Draw a tringle by drawing a single character on one line, then three on the next, ... . void DrawTriangle(int numRows){ for(int i = 1 ; i &lt;= numRows; i++) { //Print a field of spaces equal to the number of rows minus half the width of the triangle, plus 2 spaces //between the row number and the triangle, std::cout &lt;&lt; std::setfill(&#39; &#39;) &lt;&lt; std::setw(numRows - i + 2) &lt;&lt; &quot; &quot;; //Print a field of pipes equal to the number of row number*2 then subtract 1 to keep the width odd and centered. std::cout &lt;&lt; std::setfill(&#39;#&#39;) &lt;&lt; std::setw((i * 2 - 1)) &lt;&lt; &quot;#&quot;; std::cout &lt;&lt; std::endl; } } . DrawTriangle(5) . # ### ##### ####### ######### . DrawTriangle(8) . # ### ##### ####### ######### ########### ############# ############### . DrawTriangle(15); . # ### ##### ####### ######### ########### ############# ############### ################# ################### ##################### ####################### ######################### ########################### ############################# .",
            "url": "https://andersonbanihirwe.dev/blog/cpp/2018/01/20/play-interactively-with-cpp-streams.html",
            "relUrl": "/blog/cpp/2018/01/20/play-interactively-with-cpp-streams.html",
            "date": " • Jan 20, 2018"
        }
        
    
  
    
        ,"post9": {
            "title": "Play interactively with C++: Xeus-Cling",
            "content": "This is the 1st installment of a new series called Play interactively with C++. Every week or so, I’ll be summarizing and exploring Standard C++ Programming in Jupyter notebook using xeus-cling. . The source code (in notebook format) for this series can be found here. . xeus-cling is a Jupyter kernel for C++ based on the C++ interpreter cling and the native implementation of the Jupyter protocol xeus. . In this installment, I will go over how to get started with playing with C++ Programming interactively. . I am going to show you how to setup your environment. This blog post makes the following assumptions: . The reader has some experience with Jupyter notebooks. | Familiarity with conda. | Note: Credit goes to Uwe for his excellent blog post on how to setup C++ environment for Apache Arrow . Getting Started . To quote xeus-cling project: . It is preferable to install xeus-cling in a fresh conda environment. It is also needed to use a miniconda installation because with anaconda you can have a conflict with the zeromq library which is already installed with anaconda. . As a start, we create a conda environment with all non-C++ dependencies and also install Jupyter Lab from conda-forge. . # Create a new conda environment conda create -n xeus python=3.6 jupyterlab -c conda-forge source activate xeus . Finally, let&#39;s install the actual interactive environment. For the C++ support, we install the interactive C++ compiler cling and the C++ kernel for Jupyter Notebook xeus-cling from the QuantStack channel. . conda install xeus-cling -c QuantStack -c conda-forge . After starting Jupyter Lab with jupyter lab, you should now see two additional kernels:xeus C++11 and xeus C++14. You can use either of them to use write interactive C++ programs. . Now you are ready to start playing with C++ interactively. . Thanks for reading! .",
            "url": "https://andersonbanihirwe.dev/blog/cpp/2018/01/20/play-interactively-with-cpp-getting-started-with-xeus-cling.html",
            "relUrl": "/blog/cpp/2018/01/20/play-interactively-with-cpp-getting-started-with-xeus-cling.html",
            "date": " • Jan 20, 2018"
        }
        
    
  
    
        ,"post10": {
            "title": "Why Parallel Computing?",
            "content": "For many years we’ve enjoyed the fruits of ever faster processors. However, because of physical limitations the rate of performance improvement in conventional processors is decreasing. In order to increase the power of processors, chipmakers have turned to multicore integrated circuits, that is, integrated circuits with multiple conventional processors on a single chip. . . From 1986 to 2002 the performance of microprocessors increased, on average, $50 %$ per year [[1]]. Since 2002, however, single processor performance improvement has slowed to about $20 %$ per year. This difference is dramatic: at $50%$ per year, performance will increase by almost a factor of 60 in 10 years, while at $20 %$, it will only increased by about a factor of 6. . By 2005, most of the major manufacturers of microprocessors had decided that the road to rapidly increasing performance lay in the direction of parallelism. Rather than trying to continue to develop ever-faster monolithic processors, manufacturers started putting multiple complete processors on a single integrated circuit. . All of this raises a number of questions: . Why do we care? Aren&#39;t single processor systems fast enough? After all, $20 %$ per year is still a pretty significant performance improvement. . | Why can&#39;t microprocessor manufacturers continue to develop much faster single processor systems? Why build parallel systems? Why build systems with multiple processors? . | Why can&#39;t we write programs that will automatically convert serial programs into parallel programs, that is, programs that take advantage of the presence of multiple processors? . | . Let&#39;s take a brief look at each of these questions. . Why we need ever-increasing performance . The vast increases in computational power that we have been enjoying for decades now have been at the heart of many of the most dramatic advances in fields as diverse as science, the Internet, and entertainment. The following are a few examples: . Climate modeling: In order to better understand climate change, we need far more accurate computer models, models that include interactions between the atmosphere, the oceans, solid land, the ice caps at the poles. We also need to be able to make detailed studies of how various interventions might affect the global climate. | . . Protein folding: It&#39;s believed that misfolded proteins may be involved in diseases such as Huntington&#39;s disease, Parkinson&#39;s disease, Alzheimer&#39;s disease, but our ability to study configurations of complex molecules such as proteins is severely limited by our current computational power. . | Drug discovery: There are many ways in which increased computational power can be used in research into new medical treatments. For example, there are many drugs that are affective in treating a relatively small fraction of those suffering from some disease. It&#39;s possible that we can devise alternative treatments by careful analysis of the genomes of the individuals for whom the known treatment is ineffective. This, however, will involve extensive computational analysis of genomes. . | . Although the video below sheds light on how supercomputers are unraveling the mystery of the human brain by delivering the most exquisitely detailed human brain models ever created, It is a wonderful example of how ever-increasing performance is a game changer. What if instead of behavioral research, scientists could use the models to study cognition, simulate and study disease, and learn from the ways of the brain—everything from how it uses energy to how memory, representation and even consciousness itself are constructed? . . Data Analysis: We generate tremendous amounts of data. By some estimates, the quantity of data stored worldwide doubles every two years [2], but the vast majority of it is largely useless unless it is analyzed. For instance, knowing the sequence of nucleotides in human DNA is, by itself, of little use. Understanding how this sequence affects development and how it can cause disease requires extensive analysis. In addition to genomics, vast quantities of data are generated by particle colliders such as the Large Hadron Collider at CERN, medical imaging, astronomical research, and Web search engines, etc.. | . . These and a host of other problems won&#39;t be solved without vast increases in computational power. . Why we are building parallel systems . Much of the tremendous increase in single processor performance has been driven by the ever-increasing density of transistors-the electronic switches-on integrated circuits. As the size of transistors decreases, their speed can be increased, and the overall speed of the integrated circuit can be increased. However, as the speed of transistors increases, their power consumption also increases. Most of this power is dissipated as heat, and when an integrated circuit gets too hot, it becomes unreliable. Therefore, it is becoming impossible to continue to increase the speed of integrated circuits. However, the increase in transistor density can continue at least for while. . How then, can we exploit the continuing increase in transistor density? The answer is parallelism. Rather than building ever-faster, more complex, monolithic processors, the industry has decided to put multiple, relatively simple, complete processors on a single chip. Such integrated circuits are called multicore processors, and core has become synonymous with central processing unit, or CPU. . . Why we need to write parallel programs . Most programs that have been written for conventional; single-core systems cannot exploit the presence of multiple cores. We can run multiple instances of a program on a multicore system, but this is often of little help. For instance, being able to run multiple instances of our favorite game program is not really what we want - we want the program to run faster with more realistic graphics. . In order to do this, we need to either rewrite our serial programs so that they are parallel, so that they can make use of multiple cores, or write translation programs, that is, programs that will automatically convert serial programs into parallel programs. The bad news is that researchers have had very limited success writing programs that convert serial programs in languages such as C and C++ into parallel programs. . An efficient parallel implementation of a serial program may not be obtained by finding efficient parallelizations of each of its steps. Rather, the best parallelization may be obtained by stepping back and devising and entirely new algorithm. . As an example, suppose we need to compute $n$ values and add them together. This can be done with the following serial code: . total_sum = 0. for i in range(0, n): x = compute_next_value(...) total_sum += x . Now suppose we also have $p$ cores and $p$ is much smaller than $n$. Then each core can form a partial sum of approximately $ frac{n}{p}$ values: . my_sum = 0. my_first = ... my_last_i = ... for my_i in range(my_first, my_last_i): my_x = compute_next_value(...) my_sum += my_x . Here the prefix my__ indicates that each core is using its own, private variables, and each core can execute this block of code independently of the other cores. . After each core completes execution of this code, its variable my_sum will store the sum of the values computed by its calls to compute_next_value(). For instance, if there are eight cores, $n=24$, and the $24$ calls to compute_next_value() return the values . $$1, 4, 3, 9, 2, 8, 5, 1, 1, 6, 2, 7, 2, 5, 0, 4, 1, 8, 6, 5, 1, 2, 3, 9,$$ . then the values store in my_sum might be . . Here we&#39;re assuming the cores are identified by nonnegative integers in the range $0,1, dots, p-1$, where $p$ is the number of cores. . When the cores are done computing their values of my_sum, they can form a global sum by sending their results to a designated master core, which can add their results. . if (&#39;I am the master core&#39;): total_sum = my_x for each core other than myself: receive value from core total_sum += value else: send my_x to the master core . In our example, if the master core is core $0$, it would add the values $8+19+7+15+7+13+12+14=95$. . A better way to do this--especially if the number of cores is large is instead of making the master core do all the work of computing the final sum, we can pair the cores so that while core $0$ adds in the result of core $1$, core $2$ can add in the result of core $3$ and so on. Then we can repeat the process with only the even-ranked cores: $0$ adds in the result of $2$, $4$ adds in the result of $6$, and so on. Now cores divisible by $4$ repeat the process, and so on. . . This method results in an improvement of more than a factor of two. The difference becomes much more dramatic with large numbers of cores. With 1000 cores, the first method will require $999$ receives and adds, while the second will only require $10$, an improvement of almost a factor of $100$! . We might expect that software could be written so that a large number of common serial constructs could be recognized and efficiently parallelized, that is, modified so that they can use multiple cores. However, as we apply this principle to ever more complex serial programs, it becomes more and more difficult to recognize the construct, and it becomes less and less likely that we&#39;ll have a precoded, efficient parallelization. . Thus, we cannot simply continue to write serial programs, we must write parallel programs, programs that exploit the power of multiple processors. . How do we write parallel programs . There are a number of possible answers to this question, but most of them depend on the basic idea of partitioning the work to be done among the cores. There are two widely used approaches: . Task-parallelism: is the simultaneous execution on multiple cores of many different functions across the same or different datasets. | Data-parallelism (aka SIMD): is the simultaneous execution on multiple cores of the same function across the elements of a dataset. | . The first part of global sum example above would probably be considered and example of data parallelism. The data are the values computed by compute_next_value(), and each core carries out roughly the same operations on its assigned elements. . The second part of this global sum example might be considered an example of task parallelism. There are two tasks: receiving, and adding the cores&#39; partial sums, which is carried out by the master core, and giving the partial sum to the master core, which is carried out by the other cores. . In both global sum examples, the coordination involves communication: one or more cores send their current partial sums to another one. The global sum examples should also involve coordination through load balancing: even though we didn&#39;t give explicit formulas, it is clear that we want the cores all to be assigned roughly the same number of values to compute. If, for example, one core has to compute most of the values, then the other cores will finish much sooner than the heavily loaded core, and their computational power will be wasted. . A third type of coordination is synchronization. In most systems the cores are not automatically synchronized. Rather, each core works at its own pace. In this case, the problem is that we don&#39;t want the cores to race ahead and start computing their partial sums before the master is done initializing $x$ and making it available to other cores. That is, the cores need to wait before starting execution of the code. . We need to add in a point of synchronization between the initialization of $x$ and the computation of the partial sums: synchronize_cores(). . The idea here is that each core will wait in the function synchronize_cores() until all the cores have entered the function--in particular, until the master core has entered this function. . Currently, the most powerful parallel programs are written using explicit parallel constructs, that is, they are written using extensions to languages such as C and C++. The complexity of modern cores often makes it necessary to use considerable care in writing the code that will be executed by a single core. . References . [1] M. Herlihy, N. Shavit, The Art of Multiprocessor Programming, Morgan Kaufmann, Boston, 2008. . | [2] IBM, IBM InfoSphere streams v1.2.0 supports highly complex heterogeneous data analysis, IBM United States Software Announcement 210-037, February 23, 2010. Available here . | An Introduction to Parallel Programming by Peter Pacheco | .",
            "url": "https://andersonbanihirwe.dev/blog/2017/10/20/why-parallel-computing.html",
            "relUrl": "/blog/2017/10/20/why-parallel-computing.html",
            "date": " • Oct 20, 2017"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": ". About Me . My name is Anderson Banihirwe. Currently, I am working as a software engineer in Computational &amp; Information Systems Lab (CISL) at the National Center for Atmospheric Research (NCAR) while contributing to Pangeo project’s efforts. . I contribute to and maintain several libraries within the open source scientific Python stack, particularly around improving scalability of Python tools in order to handle petabyte-scale datasets on HPC and cloud platforms. Most projects that I work on can be found on my GitHub page. . I graduated from University of Arkansas at Little Rock with a Bachelor of Science degree in Systems Engineering with a focus on Computer Systems. . About This Blog . This is a place for me to share thoughts, ideas, projects with the world. If you are reading this, I am honored for taking some time out of your life. If you have read something and wish to contact me about it, please send me an email or via Twitter @andersy005, I will be happy to discuss it with you! Do contact me if you think there’s room for collaboration. . This website is powered by fastpages 1. . . Be prepared: “Luck” is truly where preparation meets opportunity. ~ Professor Randy Pausch . Look at the world differently. In a world of talkers, be a thinker and a doer. ~ Destin Sandlin . Experience is what you get when you didn’t get what you wanted. And experience is often the most valuable thing you have to offer. ~ Professor Randy Pausch . . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://andersonbanihirwe.dev/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "CV",
          "content": "",
          "url": "https://andersonbanihirwe.dev/cv/",
          "relUrl": "/cv/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
      ,"page8": {
          "title": "Talks",
          "content": "I’ve given talks at academic and software conferences. Some of these are recorded and available online. Here’s a mostly up to date list of all the talks I’ve given. . Intake-esm – Making It Easier To Consume Climate Data - CMIP6 Hackathon 2019. Boulder, CO. | JupyterHub on HPC - Pangeo Meeting 2019. Seattle, WA. August 2019 | Intake-esm - Pangeo Meeting 2019. Seattle, WA. August 2019 | Turning HPC Systems into Interactive Data Analysis Platforms using Jupyter and Dask - SciPy 2019. Austin, TX. July 2019 | Beyond Matplotlib: Building Interactive Climate Data Visualizations with Bokeh and Friends - UCAR Software Engineering Assembly. Boulder, CO. April 2018 . | PySpark for “Big” Atmospheric Data Analysis - American Meteorological Society (AMS)Conference. Austin, TX. Jan 2018 . | PySpark for “Big” Atmospheric and Oceanic Data Analysis - National Center for Atmospheric Research. Boulder, CO. Aug 2017 | .",
          "url": "https://andersonbanihirwe.dev/talks/",
          "relUrl": "/talks/",
          "date": ""
      }
      
  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://andersonbanihirwe.dev/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}